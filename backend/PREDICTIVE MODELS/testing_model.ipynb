{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522de50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-aiplatform google-cloud-bigquery pandas scikit-learn joblib xgboost --upgrade\n",
    "\n",
    "print(\"Libraries installed!\")\n",
    "# ------------------------------------------------------------------\n",
    "# CELL 2: Imports and Configuration\n",
    "# ------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    recall_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    )\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.base import clone\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# --- !!! PRODUCTION-GRADE IMPORTS !!! ---\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# -----------------------------------------\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import json\n",
    "\n",
    "# --- !!! YOUR PROJECT DETAILS !!! ---\n",
    "PROJECT_ID = \"artful-affinity-476513-t7\"\n",
    "BQ_DATASET = \"complete_db\"\n",
    "REGION = \"us-central1\"\n",
    "MODEL_DISPLAY_NAME = \"disease_outbreak_prediction_model\"\n",
    "# -------------------------------------\n",
    "\n",
    "# Initialize the Google Cloud clients\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "print(f\"Clients initialized for project {PROJECT_ID}. Ready to load data.\")\n",
    "\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime_dtype\n",
    "\n",
    "DISTRICT_RENAME_MAP = {\n",
    "    'District': 'district',\n",
    "    'district_name': 'district',\n",
    "    'DistrictName': 'district',\n",
    "    'Population': 'total_population',\n",
    "    'population': 'total_population',\n",
    "    'Patient Inflow (Daily)': 'patient_inflow_daily',\n",
    "    'patient_inflow_daily': 'patient_inflow_daily',\n",
    "    'Disease Outbreak': 'disease_outbreak',\n",
    "    'disease_outbreak': 'disease_outbreak',\n",
    "    'last_updated': 'last_updated',\n",
    "    'last_inspection_date': 'last_inspection_date',\n",
    "    'request_date': 'request_date',\n",
    "    'report_date': 'report_date',\n",
    "    'total_population': 'total_population'\n",
    "}\n",
    "\n",
    "def norm_dist(series):\n",
    "    return (\n",
    "        series.astype(str)\n",
    "              .str.strip()\n",
    "              .str.lower()\n",
    "              .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    )\n",
    "\n",
    "def norm_week(timestamp_series):\n",
    "    ts = pd.to_datetime(timestamp_series, errors='coerce')\n",
    "    try:\n",
    "        ts = ts.dt.tz_localize(None)\n",
    "    except (AttributeError, TypeError):\n",
    "        try:\n",
    "            ts = ts.dt.tz_convert(None)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return ts.dt.to_period('W-MON').dt.start_time\n",
    "\n",
    "def find_date_col(df, candidates=None):\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    if candidates is None:\n",
    "        candidates = (\n",
    "            'report_date',\n",
    "            'request_date',\n",
    "            'last_updated',\n",
    "            'inspection_date',\n",
    "            'last_inspection_date',\n",
    "            'resolution_date',\n",
    "            'date',\n",
    "            'event_date'\n",
    "        )\n",
    "    for col in candidates:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    for col in df.columns:\n",
    "        lowered = col.lower()\n",
    "        if 'date' in lowered or 'time' in lowered or 'updated' in lowered:\n",
    "            return col\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            if is_datetime_dtype(df[col]):\n",
    "                return col\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def normalize_tables():\n",
    "    for var in ['df_health','df_roads','df_safety','df_services','df_env','df_agri','df_pop']:\n",
    "        df = globals().get(var)\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        rename_candidates = {c: DISTRICT_RENAME_MAP[c] for c in df.columns if c in DISTRICT_RENAME_MAP}\n",
    "        if rename_candidates:\n",
    "            df = df.rename(columns=rename_candidates)\n",
    "        if 'district' in df.columns and 'district_norm' not in df.columns:\n",
    "            df['district_norm'] = norm_dist(df['district'])\n",
    "        date_col = find_date_col(df)\n",
    "        if date_col:\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        globals()[var] = df\n",
    "\n",
    "print('Normalization helpers ready. Call normalize_tables() after loading raw tables.')\n",
    "TABLE_MAP = {\n",
    "    'df_health': 'ai_governance_health_facilities',\n",
    "    'df_roads': 'ai_governance_infrastructure_roads',\n",
    "    'df_safety': 'ai_governance_public_safety_reports',\n",
    "    'df_services': 'ai_governance_citizen_services_requests',\n",
    "    'df_env': 'ai_governance_environment_monitoring',\n",
    "    'df_agri': 'ai_governance_agriculture_insights',\n",
    "    'df_pop': 'ai_governance_population_demographics',\n",
    "}\n",
    "\n",
    "# Optional: limit rows for quick iteration (set to None to load full table)\n",
    "ROW_LIMIT = None  # e.g., 20000 or None\n",
    "\n",
    "for varname, table in TABLE_MAP.items():\n",
    "    fq_table = f\"{PROJECT_ID}.{BQ_DATASET}.{table}\"\n",
    "    try:\n",
    "        print(f\"Loading `{fq_table}` -> {varname} ...\")\n",
    "        if ROW_LIMIT:\n",
    "            sql = f\"SELECT * FROM `{fq_table}` LIMIT {ROW_LIMIT}\"\n",
    "        else:\n",
    "            sql = f\"SELECT * FROM `{fq_table}`\"\n",
    "        job = bq_client.query(sql)\n",
    "        df = job.to_dataframe()   # may take time for big tables\n",
    "        globals()[varname] = df\n",
    "        print(f\"Loaded {varname}: shape={df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load `{fq_table}` into {varname}: {e}\")\n",
    "\n",
    "normalize_tables()\n",
    "print('Applied normalization to loaded tables (district_norm + datetime coercion).')\n",
    "\n",
    "# Quick peek\n",
    "for var in TABLE_MAP.keys():\n",
    "    if var in globals() and getattr(globals()[var], \"shape\", (0,0))[0] > 0:\n",
    "        print(f\"\\n{var} sample (first 3 rows):\")\n",
    "        display(globals()[var].head(3))\n",
    "    else:\n",
    "        print(f\"\\n{var} is empty or not found (shape={getattr(globals().get(var), 'shape', None)})\")\n",
    "def first_existing(df, candidates):\n",
    "    for column in candidates:\n",
    "        if column in df.columns:\n",
    "            return column\n",
    "    return None\n",
    "\n",
    "def parse_crime_reports(value):\n",
    "    if pd.isna(value):\n",
    "        return {}\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            parsed = json.loads(value)\n",
    "        except Exception:\n",
    "            return {}\n",
    "    elif isinstance(value, dict):\n",
    "        parsed = [value]\n",
    "    else:\n",
    "        parsed = value\n",
    "    if not isinstance(parsed, (list, tuple)):\n",
    "        return {}\n",
    "    counts = {}\n",
    "    for item in parsed:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "        crime_type = item.get('type')\n",
    "        count = item.get('count', 1)\n",
    "        try:\n",
    "            count = int(float(count))\n",
    "        except Exception:\n",
    "            count = 0\n",
    "        if crime_type and count:\n",
    "            counts[crime_type] = counts.get(crime_type, 0) + count\n",
    "    return counts\n",
    "\n",
    "for suffix in ['health', 'env', 'safety', 'services', 'roads', 'agri', 'pop']:\n",
    "    var_name = f'df_{suffix}'\n",
    "    if var_name not in globals():\n",
    "        globals()[var_name] = pd.DataFrame()\n",
    "\n",
    "date_cols = {}\n",
    "for suffix in ['health', 'env', 'safety', 'services', 'roads', 'agri']:\n",
    "    df = globals()[f'df_{suffix}']\n",
    "    if not df.empty and 'district_norm' not in df.columns and 'district' in df.columns:\n",
    "        df['district_norm'] = norm_dist(df['district'])\n",
    "    date_col = find_date_col(df)\n",
    "    date_cols[suffix] = date_col\n",
    "    if date_col:\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    globals()[f'df_{suffix}'] = df\n",
    "\n",
    "if not df_pop.empty and 'district_norm' not in df_pop.columns and 'district' in df_pop.columns:\n",
    "    df_pop['district_norm'] = norm_dist(df_pop['district'])\n",
    "\n",
    "# Health weekly aggregation\n",
    "health_week = pd.DataFrame()\n",
    "if not df_health.empty and date_cols.get('health'):\n",
    "    health_date = date_cols['health']\n",
    "    patient_col = first_existing(df_health, ['patient_inflow_daily', 'patient_inflow', 'patient_inflow_mean'])\n",
    "    outbreak_col = first_existing(df_health, ['disease_outbreak'])\n",
    "    health_cols = ['district_norm', health_date]\n",
    "    if patient_col:\n",
    "        health_cols.append(patient_col)\n",
    "    if outbreak_col:\n",
    "        health_cols.append(outbreak_col)\n",
    "    h = df_health[health_cols].copy()\n",
    "    h = h.dropna(subset=['district_norm'])\n",
    "    h['week_start'] = norm_week(h[health_date])\n",
    "    h = h.dropna(subset=['week_start'])\n",
    "    h['health_events'] = 1\n",
    "    agg_map = {'health_events': 'sum'}\n",
    "    if patient_col:\n",
    "        h['patient_inflow_value'] = pd.to_numeric(h[patient_col], errors='coerce')\n",
    "        agg_map['patient_inflow_value'] = 'mean'\n",
    "    if outbreak_col:\n",
    "        outbreak_series = h[outbreak_col].astype(str).str.strip().str.lower()\n",
    "        h['outbreak_flag'] = (~outbreak_series.isin({'', 'none', 'null', 'nan'})).astype(int)\n",
    "        agg_map['outbreak_flag'] = 'sum'\n",
    "    health_week = h.groupby(['district_norm', 'week_start']).agg(agg_map).reset_index()\n",
    "    if 'patient_inflow_value' in health_week.columns:\n",
    "        health_week = health_week.rename(columns={'patient_inflow_value': 'patient_inflow_mean'})\n",
    "    if 'outbreak_flag' in health_week.columns:\n",
    "        health_week = health_week.rename(columns={'outbreak_flag': 'outbreak_count'})\n",
    "    else:\n",
    "        health_week['outbreak_count'] = 0\n",
    "    if 'health_events' not in health_week.columns:\n",
    "        health_week['health_events'] = 0\n",
    "\n",
    "# Environment weekly aggregation\n",
    "env_week = pd.DataFrame()\n",
    "if not df_env.empty and date_cols.get('env'):\n",
    "    env_date = date_cols['env']\n",
    "    env_metrics = [c for c in ['air_quality_index', 'pm25_level', 'pm10_level', 'water_quality_index', 'waste_collection_efficiency'] if c in df_env.columns]\n",
    "    if env_metrics:\n",
    "        env_cols = ['district_norm', env_date] + env_metrics\n",
    "        e = df_env[env_cols].copy()\n",
    "        e = e.dropna(subset=['district_norm'])\n",
    "        e['week_start'] = norm_week(e[env_date])\n",
    "        e = e.dropna(subset=['week_start'])\n",
    "        agg_map = {c: 'mean' for c in env_metrics}\n",
    "        env_week = e.groupby(['district_norm', 'week_start']).agg(agg_map).reset_index()\n",
    "\n",
    "# Services weekly aggregation\n",
    "services_week = pd.DataFrame()\n",
    "if not df_services.empty and date_cols.get('services'):\n",
    "    services_date = date_cols['services']\n",
    "    service_cols = ['district_norm', services_date]\n",
    "    if 'service_type' in df_services.columns:\n",
    "        service_cols.append('service_type')\n",
    "    svc = df_services[service_cols].copy()\n",
    "    svc = svc.dropna(subset=['district_norm'])\n",
    "    svc['week_start'] = norm_week(svc[services_date])\n",
    "    svc = svc.dropna(subset=['week_start'])\n",
    "    svc['services_events'] = 1\n",
    "    agg_map = {'services_events': 'sum'}\n",
    "    if 'service_type' in svc.columns:\n",
    "        agg_map['service_type'] = 'count'\n",
    "    services_week = svc.groupby(['district_norm', 'week_start']).agg(agg_map).reset_index()\n",
    "    if 'service_type' in services_week.columns:\n",
    "        services_week = services_week.rename(columns={'service_type': 'complaint_count'})\n",
    "\n",
    "# Safety weekly aggregation from crime JSON\n",
    "safety_week = pd.DataFrame()\n",
    "if not df_safety.empty and date_cols.get('safety') and 'crime_reports' in df_safety.columns:\n",
    "    safety_date = date_cols['safety']\n",
    "    safe = df_safety[['district_norm', safety_date, 'crime_reports']].copy()\n",
    "    safe = safe.dropna(subset=['district_norm'])\n",
    "    safe['week_start'] = norm_week(safe[safety_date])\n",
    "    safe = safe.dropna(subset=['week_start'])\n",
    "    safe['crime_counts'] = safe['crime_reports'].apply(parse_crime_reports)\n",
    "    crime_rows = []\n",
    "    for _, row in safe.iterrows():\n",
    "        counts = row['crime_counts'] or {}\n",
    "        for crime_type, count in counts.items():\n",
    "            crime_rows.append({'district_norm': row['district_norm'], 'week_start': row['week_start'], 'crime_type': crime_type, 'count': count})\n",
    "    if crime_rows:\n",
    "        crime_df = pd.DataFrame(crime_rows)\n",
    "        safety_week = (\n",
    "            crime_df.groupby(['district_norm', 'week_start', 'crime_type'])['count']\n",
    "            .sum()\n",
    "            .unstack(fill_value=0)\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "for label, wk in [('health', health_week), ('env', env_week), ('services', services_week), ('safety', safety_week)]:\n",
    "    if wk is None or wk.empty:\n",
    "        print(f'{label}_week: empty')\n",
    "    else:\n",
    "        print(\n",
    "            f\"{label}_week rows={len(wk)}, districts={wk['district_norm'].nunique()}, \"\n",
    "            f\"weeks={wk['week_start'].min()}->{wk['week_start'].max()}\"\n",
    "        )\n",
    "\n",
    "weekly_tables = [health_week, env_week, services_week, safety_week]\n",
    "\n",
    "def _uniq(series):\n",
    "    return set(series.dropna().unique().tolist())\n",
    "\n",
    "signal_districts = set()\n",
    "week_union = set()\n",
    "for wk in weekly_tables:\n",
    "    if wk is not None and not wk.empty:\n",
    "        signal_districts |= _uniq(wk['district_norm'])\n",
    "        week_union |= _uniq(wk['week_start'])\n",
    "\n",
    "if df_pop.empty:\n",
    "    raise ValueError('Population table is required to build the panel.')\n",
    "\n",
    "if 'district_norm' not in df_pop.columns and 'district' in df_pop.columns:\n",
    "    df_pop['district_norm'] = norm_dist(df_pop['district'])\n",
    "\n",
    "pop_districts = _uniq(df_pop['district_norm'])\n",
    "missing_in_pop = sorted(signal_districts - pop_districts)\n",
    "if missing_in_pop:\n",
    "    preview = ', '.join(missing_in_pop[:8])\n",
    "    print(f'Districts with weekly signal missing in population ({len(missing_in_pop)}): {preview}')\n",
    "missing_signal = sorted(pop_districts - signal_districts)\n",
    "print(f'Districts with population but no weekly signal: {len(missing_signal)}')\n",
    "\n",
    "districts = sorted(signal_districts & pop_districts)\n",
    "weeks = sorted(week_union)\n",
    "\n",
    "if not districts or not weeks:\n",
    "    raise ValueError('No overlapping districts/weeks between population and weekly tables after normalization.')\n",
    "\n",
    "panel = pd.MultiIndex.from_product([districts, weeks], names=['district_norm', 'week_start']).to_frame(index=False)\n",
    "\n",
    "def merge_weekly(base, weekly):\n",
    "    if weekly is None or weekly.empty:\n",
    "        return base\n",
    "    temp = weekly.copy()\n",
    "    temp['week_start'] = pd.to_datetime(temp['week_start']).dt.floor('D')\n",
    "    return base.merge(temp, on=['district_norm', 'week_start'], how='left')\n",
    "\n",
    "panel['week_start'] = pd.to_datetime(panel['week_start']).dt.floor('D')\n",
    "panel = merge_weekly(panel, health_week)\n",
    "panel = merge_weekly(panel, env_week)\n",
    "panel = merge_weekly(panel, services_week)\n",
    "panel = merge_weekly(panel, safety_week)\n",
    "\n",
    "def inner_hits(base, weekly):\n",
    "    if weekly is None or weekly.empty:\n",
    "        return 0\n",
    "    pairs = weekly[['district_norm', 'week_start']].dropna().drop_duplicates()\n",
    "    pairs['week_start'] = pd.to_datetime(pairs['week_start']).dt.floor('D')\n",
    "    return base[['district_norm', 'week_start']].merge(pairs, how='inner').shape[0]\n",
    "\n",
    "print('Inner hits with weekly tables:',\n",
    "      inner_hits(panel, health_week),\n",
    "      inner_hits(panel, env_week),\n",
    "      inner_hits(panel, services_week),\n",
    "      inner_hits(panel, safety_week))\n",
    "\n",
    "pop_cols = [c for c in ['district_norm', 'district', 'total_population', 'population_density_per_sqkm', 'avg_household_size'] if c in df_pop.columns]\n",
    "pop_frame = df_pop[pop_cols].drop_duplicates(subset=['district_norm'])\n",
    "panel = panel.merge(pop_frame, on='district_norm', how='left')\n",
    "\n",
    "if 'district' not in panel.columns:\n",
    "    panel['district'] = panel['district_norm']\n",
    "\n",
    "if 'total_population' in panel.columns:\n",
    "    panel['total_population'] = pd.to_numeric(panel['total_population'], errors='coerce')\n",
    "    panel['pop_per_100k'] = panel['total_population'].replace({0: np.nan}) / 100000.0\n",
    "else:\n",
    "    panel['pop_per_100k'] = np.nan\n",
    "\n",
    "count_cols = [c for c in ['outbreak_count', 'health_events', 'services_events', 'complaint_count'] if c in panel.columns]\n",
    "for col in count_cols:\n",
    "    panel[col] = pd.to_numeric(panel[col], errors='coerce').fillna(0)\n",
    "\n",
    "crime_cols = [c for c in panel.columns if c.startswith('crime_')]\n",
    "for col in crime_cols:\n",
    "    panel[col] = pd.to_numeric(panel[col], errors='coerce').fillna(0)\n",
    "    panel[col] = np.where(panel['pop_per_100k'] > 0, panel[col] / panel['pop_per_100k'], 0)\n",
    "\n",
    "cont_candidates = [\n",
    "    'patient_inflow_mean',\n",
    "    'air_quality_index',\n",
    "    'pm25_level',\n",
    "    'pm10_level',\n",
    "    'water_quality_index',\n",
    "    'waste_collection_efficiency',\n",
    "    'traffic_volume_daily',\n",
    "    'condition_score'\n",
    "    ]\n",
    "cont_cols = [c for c in cont_candidates if c in panel.columns]\n",
    "for col in cont_cols:\n",
    "    panel[col] = pd.to_numeric(panel[col], errors='coerce')\n",
    "    panel[col] = panel.groupby('district_norm')[col].transform(lambda s: s.ffill().bfill())\n",
    "    median_val = panel[col].median(skipna=True)\n",
    "    panel[col] = panel[col].fillna(median_val)\n",
    "\n",
    "for window in [1, 2, 4]:\n",
    "    if 'pm25_level' in panel.columns:\n",
    "        panel[f'pm25_roll_{window}w'] = panel.groupby('district_norm')['pm25_level'].transform(\n",
    "            lambda s: s.shift(1).rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "    if 'patient_inflow_mean' in panel.columns:\n",
    "        panel[f'patient_inflow_roll_{window}w'] = panel.groupby('district_norm')['patient_inflow_mean'].transform(\n",
    "            lambda s: s.shift(1).rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "def future_sum(series, horizon):\n",
    "    values = series.fillna(0).to_numpy()\n",
    "    out = np.zeros(len(values), dtype=float)\n",
    "    for idx in range(len(values)):\n",
    "        start = idx + 1\n",
    "        end = min(idx + 1 + horizon, len(values))\n",
    "        out[idx] = values[start:end].sum()\n",
    "    return pd.Series(out, index=series.index)\n",
    "\n",
    "if 'outbreak_count' not in panel.columns:\n",
    "    panel['outbreak_count'] = 0\n",
    "panel['outbreak_count'] = pd.to_numeric(panel['outbreak_count'], errors='coerce').fillna(0)\n",
    "\n",
    "HORIZON_WEEKS = 2\n",
    "panel = panel.sort_values(['district_norm', 'week_start']).reset_index(drop=True)\n",
    "panel['outbreak_next_14d'] = (\n",
    "    panel.groupby('district_norm')['outbreak_count']\n",
    "         .transform(lambda s: future_sum(s, HORIZON_WEEKS))\n",
    "         .gt(0)\n",
    "         .astype(int)\n",
    ")\n",
    "\n",
    "positive_rows = int((panel['outbreak_count'] > 0).sum())\n",
    "positive_rate = float(panel['outbreak_next_14d'].mean())\n",
    "print('Panel districts:', len(districts))\n",
    "print('Panel weeks:', len(weeks))\n",
    "print('Rows with outbreak_count>0:', positive_rows)\n",
    "print('Positive rate (outbreak_next_14d):', positive_rate)\n",
    "\n",
    "if not env_week.empty and len(panel) > 0:\n",
    "    env_share = (\n",
    "        panel[['district_norm', 'week_start']]\n",
    "        .merge(env_week[['district_norm', 'week_start']].drop_duplicates(), how='inner')\n",
    "        .shape[0] / len(panel)\n",
    "    )\n",
    "    print(f'Share of rows with raw env coverage: {env_share:.3f}')\n",
    "elif env_week.empty:\n",
    "    print('Share of rows with raw env coverage: 0.000')\n",
    "\n",
    "df_model = panel.copy()\n",
    "df_model.to_csv('district_week_panel_demo.csv', index=False)\n",
    "print('Saved CLEAN district_week_panel_demo.csv (normalized panel).')\n",
    "# 1) Compare district vocabularies\n",
    "print(\"pop districts (sample):\", df_pop['district'].dropna().unique()[:10])\n",
    "print(\"health districts (sample):\", df_health['district'].dropna().unique()[:10])\n",
    "\n",
    "# 2) How many overlaps?\n",
    "pop_set = set(df_pop['district'].dropna().str.strip().str.lower())\n",
    "health_set = set(df_health['district'].dropna().str.strip().str.lower())\n",
    "print(\"overlap count:\", len(pop_set & health_set))\n",
    "\n",
    "# Inspect health / env columns & dtypes\n",
    "print(\"df_health columns:\", df_health.columns.tolist())\n",
    "print(df_health.dtypes)\n",
    "display(df_health.head(5))\n",
    "\n",
    "print(\"df_env columns:\", df_env.columns.tolist())\n",
    "print(df_env.dtypes)\n",
    "display(df_env.head(5))\n",
    "\n",
    "cands = ['patient_inflow','patient_inflow_daily','Patient Inflow (Daily)','patient_inflow_mean']\n",
    "for c in cands:\n",
    "    if c in df_health.columns:\n",
    "        print(c, \"non-null fraction:\", df_health[c].notna().mean(), \"dtype:\", df_health[c].dtype)\n",
    "        \n",
    "for c in ['disease_outbreak','Disease Outbreak','diseaseOutbreak']:\n",
    "    if c in df_health.columns:\n",
    "        print(c, \"unique values:\", df_health[c].astype(str).value_counts(dropna=False).head(10))\n",
    "        \n",
    "def coverage(panel, cols):\n",
    "    return panel.groupby('district')[cols].apply(lambda s: s.notna().mean()).head(10)\n",
    "print(\"Coverage sample (env + health):\")\n",
    "print(coverage(panel, [c for c in ['pm25_level','patient_inflow_mean','air_quality_index'] if c in panel.columns]))\n",
    "print(\"Outbreak positives:\", panel['outbreak_next_14d'].sum(), \" / \", len(panel))\n",
    "# --- Create df_merged skeleton from the weekly panel so downstream joins work ---\n",
    "# Place this right after you create `panel` and before CELL 4.1\n",
    "\n",
    "df_merged = (\n",
    "    panel[['district', 'district_norm', 'week_start']]\n",
    "    .rename(columns={'week_start': 'feature_date'})\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# If `district` might be missing in panel, backfill from district_norm\n",
    "if 'district' not in df_merged.columns:\n",
    "    df_merged['district'] = df_merged['district_norm']\n",
    "\n",
    "# Ensure proper dtypes\n",
    "df_merged['feature_date'] = pd.to_datetime(df_merged['feature_date'], errors='coerce')\n",
    "df_merged['district_norm'] = norm_dist(df_merged['district_norm'])\n",
    "# ------------------------------------------------------------------\n",
    " # CELL 4.1: Create forward-looking target & windowed features (Horizon)\n",
    " # ------------------------------------------------------------------\n",
    "import warnings\n",
    "from datetime import timedelta\n",
    "HORIZON_DAYS = 14  # predict outbreak within next 14 days\n",
    "# 1) Forward-looking target from health events\n",
    "date_col = find_date_col(df_health)\n",
    "outbreak_col = first_existing(df_health, ['disease_outbreak', 'Disease Outbreak'])\n",
    "if date_col is None or outbreak_col is None:\n",
    "    warnings.warn('Health table lacks a usable date or outbreak column; skipping forward-looking label build.')\n",
    "elif 'df_merged' not in globals():\n",
    "    warnings.warn('df_merged is not defined; cannot project forward-looking label onto merged feature table.')\n",
    "else:\n",
    "    df_health[date_col] = pd.to_datetime(df_health[date_col], errors='coerce')\n",
    "    outbreak_series = df_health[outbreak_col].astype(str).str.strip().str.lower()\n",
    "    valid_mask = ~outbreak_series.isin({'', 'none', 'null', 'nan'})\n",
    "    df_events = df_health[valid_mask][['district', 'district_norm', date_col]].copy()\n",
    "    df_events = df_events.rename(columns={date_col: 'report_date'})\n",
    "    def has_future_outbreak(row):\n",
    "        d_norm = row.get('district_norm')\n",
    "        if d_norm is None or pd.isna(d_norm):\n",
    "            d_norm = norm_dist(pd.Series([row.get('district', '')])).iloc[0]\n",
    "        t0 = row.get('feature_date')\n",
    "        if d_norm is None or pd.isna(t0):\n",
    "            return 0\n",
    "        ev = df_events[df_events['district_norm'] == d_norm]['report_date']\n",
    "        if ev.empty:\n",
    "            return 0\n",
    "        future = ev[(ev > t0) & (ev <= t0 + pd.Timedelta(days=HORIZON_DAYS))]\n",
    "        return int(not future.empty)\n",
    "    df_merged['district_norm'] = norm_dist(df_merged['district']) if 'district_norm' not in df_merged.columns else df_merged['district_norm']\n",
    "    df_merged['outbreak_future_14d'] = df_merged.apply(has_future_outbreak, axis=1)\n",
    "    df_merged['outbreak_risk'] = df_merged['outbreak_future_14d']\n",
    "    print(f\"Created forward-looking label outbreak_future_14d using {date_col} / {outbreak_col}\")\n",
    "# 2) Parse crime JSON into district-level columns (no week, district-level)\n",
    "if 'crime_reports' in df_safety.columns and 'df_merged' in globals():\n",
    "    safe = df_safety[['district_norm', 'crime_reports']].dropna(subset=['district_norm']).copy()\n",
    "    safe['crime_counts'] = safe['crime_reports'].apply(parse_crime_reports)\n",
    "\n",
    "    # explode into long rows: (district_norm, crime_type, count)\n",
    "    rows = []\n",
    "    for _, r in safe.iterrows():\n",
    "        cc = r['crime_counts'] or {}\n",
    "        for ctype, cnt in cc.items():\n",
    "            rows.append({'district_norm': r['district_norm'], 'crime_type': ctype, 'count': int(cnt)})\n",
    "    if rows:\n",
    "        crime_df = pd.DataFrame(rows)\n",
    "        # aggregate across all safety rows per district\n",
    "        crime_wide = (\n",
    "            crime_df.groupby(['district_norm', 'crime_type'])['count']\n",
    "                    .sum()\n",
    "                    .unstack(fill_value=0)\n",
    "                    .add_prefix('crime_')\n",
    "                    .reset_index()\n",
    "        )\n",
    "        # merge once (unique index ensured by reset_index)\n",
    "        df_merged = df_merged.merge(crime_wide, on='district_norm', how='left')\n",
    "        # fill NaNs from left-join\n",
    "        crime_cols = [c for c in df_merged.columns if c.startswith('crime_')]\n",
    "        df_merged[crime_cols] = df_merged[crime_cols].fillna(0).astype(int)\n",
    "        print(f\"Merged crime JSON counts into df_merged ({len(crime_cols)} columns).\")\n",
    "    else:\n",
    "        print(\"crime_reports present but no parsable rows.\")\n",
    "elif 'df_merged' not in globals():\n",
    "    warnings.warn('df_merged is not defined; skipping crime_reports feature engineering.')\n",
    "else:\n",
    "    print('No crime_reports column available to parse.')\n",
    "# 3) Placeholder for rolling environmental joins (requires df_merged with feature_date)\n",
    "env_date_col = find_date_col(df_env)\n",
    "if env_date_col and 'df_merged' in globals():\n",
    "    df_env[env_date_col] = pd.to_datetime(df_env[env_date_col], errors='coerce')\n",
    "    env_weekly = df_env.copy()\n",
    "    env_weekly['week_start'] = norm_week(env_weekly[env_date_col])\n",
    "    env_metrics = env_weekly.groupby(['district_norm', 'week_start']).agg({\n",
    "        'pm25_level': 'mean',\n",
    "        'pm10_level': 'mean',\n",
    "        'air_quality_index': 'mean'\n",
    "    }).reset_index()\n",
    "    df_merged['feature_week'] = norm_week(df_merged['feature_date'])\n",
    "    df_merged = df_merged.merge(\n",
    "        env_metrics.rename(columns={'week_start': 'feature_week'}),\n",
    "        on=['district_norm', 'feature_week'],\n",
    "        how='left'\n",
    "    )\n",
    "    print('Merged weekly environmental metrics onto df_merged (demo).')\n",
    "elif 'df_merged' not in globals():\n",
    "    warnings.warn('df_merged missing; skipping environmental feature join.')\n",
    "else:\n",
    "    print('No date column in environment_monitoring; skipping environmental window features.')\n",
    "print('Horizon & windowed feature step complete (demo skeleton).')\n",
    "# ------------------------------------------------------------------\n",
    "# CELL 5: Preprocess Data (NO FEATURE SELECTION)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Select limited, meaningful features for modeling (as-of at week_start)\n",
    "relevant_features = [\n",
    "    'pm25_level', 'pm10_level', 'air_quality_index', 'water_quality_index', 'waste_collection_efficiency',\n",
    "    'population_density_per_sqkm', 'avg_household_size',\n",
    "    'patient_inflow_mean',\n",
    "    # rolling features\n",
    "    'pm25_roll_1w', 'pm25_roll_2w', 'pm25_roll_4w',\n",
    "    'patient_inflow_roll_1w', 'patient_inflow_roll_2w', 'patient_inflow_roll_4w'\n",
    "    ]\n",
    "\n",
    "# Ensure features exist\n",
    "relevant_features = [f for f in relevant_features if f in df_model.columns]\n",
    "print('--- Using all 14 features: ---')\n",
    "print(relevant_features)\n",
    "\n",
    "# Target\n",
    "target_col = 'outbreak_next_14d'\n",
    "X_df = df_model[['district', 'week_start'] + relevant_features].copy()\n",
    "y = df_model[target_col].copy()\n",
    "\n",
    "# Drop initial rows with NaN in key features (e.g., from rolling windows)\n",
    "X_df = X_df.dropna(subset=relevant_features)\n",
    "# align y\n",
    "y = y.loc[X_df.index]\n",
    "\n",
    "# Split by time: train up to a date, test after\n",
    "cutoff = X_df['week_start'].quantile(0.8)\n",
    "train_mask = X_df['week_start'] <= cutoff\n",
    "X_train_raw = X_df[train_mask].drop(columns=['district', 'week_start'])\n",
    "X_test_raw = X_df[~train_mask].drop(columns=['district', 'week_start'])\n",
    "y_train = y[train_mask]\n",
    "y_test = y[~train_mask]\n",
    "\n",
    "print(f'\\nTrain weeks: {X_df[train_mask][\"week_start\"].min()} to {X_df[train_mask][\"week_start\"].max()}')\n",
    "print(f'Test weeks: {X_df[~train_mask][\"week_start\"].min()} to {X_df[~train_mask][\"week_start\"].max()}')\n",
    "\n",
    "# --- This is our full feature list ---\n",
    "numerical_cols = X_train_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Training with {len(numerical_cols)} features.\")\n",
    "\n",
    "# --- This is our preprocessor ---\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "# We only have numeric features, so the preprocessor is simple\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols)])\n",
    "\n",
    "print('Final feature matrix shapes (before model pipeline):', X_train_raw.shape, X_test_raw.shape)\n",
    "print('Train positive rate:', y_train.mean(), 'Test positive rate:', y_test.mean())\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CELL 6: Data Exploration (Split moved to CELL 5)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "print(\"Time-based data split completed in CELL 5 to avoid data leakage.\")\n",
    "print(f\"Train period: {X_df[train_mask]['week_start'].min()} to {X_df[train_mask]['week_start'].max()}\")\n",
    "print(f\"Test period: {X_df[~train_mask]['week_start'].min()} to {X_df[~train_mask]['week_start'].max()}\")\n",
    "print('Number of districts in panel:', df_model['district'].nunique())\n",
    "print('Date range in panel:', df_model['week_start'].min(), 'to', df_model['week_start'].max())\n",
    "\n",
    "best_model = None\n",
    "# ------------------------------------------------------------------\n",
    "# CELL 7: Choose Best Algorithm (with ReshapeTo2D Fix)\n",
    "# ------------------------------------------------------------------\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# --- !!! YEH HAI FINAL FIX !!! ---\n",
    "# Yeh custom class Vertex AI ke 1D array bug ko fix karta hai\n",
    "class ReshapeTo2D(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Fixes the Vertex AI container bug that passes a 1D array to a pipeline\n",
    "    that was trained on a 2D DataFrame.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self # Isse kuch seekhna nahin hai\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Check karein ki X ek 1D list/array hai ya nahin\n",
    "        if len(getattr(X, 'shape', [])) == 1:\n",
    "            # Use 2D array (single sample) mein reshape karein\n",
    "            return np.array(X).reshape(1, -1)\n",
    "\n",
    "        # Agar pehle se hi 2D hai (jaise training ke time), toh use pass karein\n",
    "        return X\n",
    "# -----------------------------------\n",
    "\n",
    "# Compute class weight / scale\n",
    "pos = y_train.sum()\n",
    "neg = len(y_train) - pos\n",
    "scale_pos_weight = (neg / pos) if pos > 0 else 1.0\n",
    "print('scale_pos_weight (neg/pos):', scale_pos_weight)\n",
    "\n",
    "models_to_test = {\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        class_weight='balanced',\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'XGBoost': xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "best_model_estimator = None\n",
    "best_score = -np.inf\n",
    "best_name = ''\n",
    "\n",
    "for name, model_estimator in models_to_test.items():\n",
    "\n",
    "    # --- Pipeline mein 'ReshapeTo2D' ko Step 1 banayein ---\n",
    "    test_pipeline = Pipeline([\n",
    "        ('reshape_fix', ReshapeTo2D()),    # <-- FIX\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model_estimator)\n",
    "    ])\n",
    "\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    test_pipeline.fit(X_train_raw, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = test_pipeline.predict(X_test_raw)\n",
    "    y_proba = test_pipeline.predict_proba(X_test_raw)[:, 1]\n",
    "\n",
    "    pr_auc = average_precision_score(y_test, y_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    print(f\"--- {name} Results ---\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PR AUC (Area Under Precision-Recall Curve): {pr_auc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"No Outbreak\", \"Outbreak\"], zero_division=0))\n",
    "\n",
    "    score = pr_auc\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model_estimator = model_estimator\n",
    "        best_name = name\n",
    "\n",
    "print(f\"\\nBest model: {best_name} with PR AUC {best_score:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CELL 8: Train Final Model and Save (with ReshapeTo2D Fix)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Hum wrapper class ko yahaan phir se define karte hain taaki yeh cell\n",
    "# akele bhi run ho sake\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "class ReshapeTo2D(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if len(getattr(X, 'shape', [])) == 1:\n",
    "            return np.array(X).reshape(1, -1)\n",
    "        return X\n",
    "\n",
    "if 'best_model_estimator' in locals():\n",
    "    print(f\"Building final pipeline with best model: {best_name}\")\n",
    "\n",
    "    # --- Final pipeline mein 'ReshapeTo2D' ko Step 1 banayein ---\n",
    "    full_pipeline = Pipeline([\n",
    "        ('reshape_fix', ReshapeTo2D()),    # <-- FIX\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', best_model_estimator)\n",
    "    ])\n",
    "\n",
    "    # Fit on full training set\n",
    "    full_pipeline.fit(X_train_raw, y_train)\n",
    "\n",
    "    # Save artifact at repo root\n",
    "    import joblib\n",
    "    joblib.dump(full_pipeline, 'model.joblib')\n",
    "    print('Saved final pipeline as model.joblib')\n",
    "\n",
    "else:\n",
    "    print(\"Error: No best model was selected. Cannot build final pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6964ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# CELL 10: Deploy Model to a Vertex AI Endpoint\n",
    "# ------------------------------------------------------------------\n",
    "# Yeh naya, healthy \"station\" banayega.\n",
    "# (Ismein 10-15 minute lagenge)\n",
    "\n",
    "if 'vertex_model' in locals():\n",
    "    print(\"Deploying model to an Endpoint... (This may take 10-15 minutes)\")\n",
    "    \n",
    "    endpoint = vertex_model.deploy(\n",
    "        machine_type=\"n1-standard-2\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDeployment complete!\")\n",
    "    print(f\"\\n--- !!! YEH AAPKA NAYA ID HAI !!! ---\")\n",
    "    print(f\"NEW ENDPOINT_ID: {endpoint.name}\") # e.g., \"1234567890123456789\"\n",
    "    print(\"--- !!! ISE SAVE KAREIN !!! ---\")\n",
    "else:\n",
    "    print(\"Skipping deployment... Run Cell 9 first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
