{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af0c7698-deec-483b-9d40-37ef57d2dd57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.124.0)\n",
      "Requirement already satisfied: google-cloud-bigquery in /opt/conda/lib/python3.10/site-packages (3.38.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.10/site-packages (3.1.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.26.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.41.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (6.31.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (25.0)\n",
      "Requirement already satisfied: google-cloud-storage<4.0.0,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.14.2)\n",
      "Requirement already satisfied: shapely<3.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.1.2)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.42.0)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.12.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (4.15.0)\n",
      "Requirement already satisfied: docstring_parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.17.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery) (2.32.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.75.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.75.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (4.9.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<4.0.0,>=1.32.0->google-cloud-aiplatform) (1.7.1)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (4.11.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.28.1)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (15.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0->google-cloud-aiplatform) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /opt/conda/lib/python3.10/site-packages (from xgboost) (2.28.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Libraries installed!\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-aiplatform google-cloud-bigquery pandas scikit-learn joblib xgboost --upgrade\n",
    "\n",
    "print(\"Libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d298fc79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clients initialized for project artful-affinity-476513-t7. Ready to load data.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# CELL 2: Imports and Configuration\n",
    "# ------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# --- !!! PRODUCTION-GRADE IMPORTS !!! ---\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# -----------------------------------------\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import json\n",
    "\n",
    "# --- !!! YOUR PROJECT DETAILS !!! ---\n",
    "PROJECT_ID = \"artful-affinity-476513-t7\"\n",
    "BQ_DATASET = \"complete_db\"\n",
    "REGION = \"us-central1\"\n",
    "MODEL_DISPLAY_NAME = \"disease_outbreak_prediction_model\"\n",
    "# -------------------------------------\n",
    "\n",
    "# Initialize the Google Cloud clients\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "print(f\"Clients initialized for project {PROJECT_ID}. Ready to load data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718e426-21ae-454d-bf4b-8bd332d8c3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema normalization complete. Check printed warnings and df_* column lists if any.\n"
     ]
    }
   ],
   "source": [
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime_dtype\n",
    "\n",
    "DISTRICT_RENAME_MAP = {\n",
    "    'District': 'district',\n",
    "    'district_name': 'district',\n",
    "    'DistrictName': 'district',\n",
    "    'Population': 'total_population',\n",
    "    'population': 'total_population',\n",
    "    'Patient Inflow (Daily)': 'patient_inflow_daily',\n",
    "    'patient_inflow_daily': 'patient_inflow_daily',\n",
    "    'Disease Outbreak': 'disease_outbreak',\n",
    "    'disease_outbreak': 'disease_outbreak',\n",
    "    'last_updated': 'last_updated',\n",
    "    'last_inspection_date': 'last_inspection_date',\n",
    "    'request_date': 'request_date',\n",
    "    'report_date': 'report_date',\n",
    "    'total_population': 'total_population'\n",
    "}\n",
    "\n",
    "def norm_dist(series):\n",
    "    return (\n",
    "        series.astype(str)\n",
    "              .str.strip()\n",
    "              .str.lower()\n",
    "              .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    )\n",
    "\n",
    "def norm_week(timestamp_series):\n",
    "    ts = pd.to_datetime(timestamp_series, errors='coerce')\n",
    "    try:\n",
    "        ts = ts.dt.tz_localize(None)\n",
    "    except (AttributeError, TypeError):\n",
    "        try:\n",
    "            ts = ts.dt.tz_convert(None)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return ts.dt.to_period('W-MON').dt.start_time\n",
    "\n",
    "def find_date_col(df, candidates=None):\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    if candidates is None:\n",
    "        candidates = (\n",
    "            'report_date',\n",
    "            'request_date',\n",
    "            'last_updated',\n",
    "            'inspection_date',\n",
    "            'last_inspection_date',\n",
    "            'resolution_date',\n",
    "            'date',\n",
    "            'event_date'\n",
    "        )\n",
    "    for col in candidates:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    for col in df.columns:\n",
    "        lowered = col.lower()\n",
    "        if 'date' in lowered or 'time' in lowered or 'updated' in lowered:\n",
    "            return col\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            if is_datetime_dtype(df[col]):\n",
    "                return col\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def normalize_tables():\n",
    "    for var in ['df_health','df_roads','df_safety','df_services','df_env','df_agri','df_pop']:\n",
    "        df = globals().get(var)\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        rename_candidates = {c: DISTRICT_RENAME_MAP[c] for c in df.columns if c in DISTRICT_RENAME_MAP}\n",
    "        if rename_candidates:\n",
    "            df = df.rename(columns=rename_candidates)\n",
    "        if 'district' in df.columns and 'district_norm' not in df.columns:\n",
    "            df['district_norm'] = norm_dist(df['district'])\n",
    "        date_col = find_date_col(df)\n",
    "        if date_col:\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        globals()[var] = df\n",
    "\n",
    "print('Normalization helpers ready. Call normalize_tables() after loading raw tables.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2036b299-fa4c-4c84-96d7-d077198d2069",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading `artful-affinity-476513-t7.complete_db.ai_governance_health_facilities` -> df_health ...\n",
      "Loaded df_health: shape=(221, 9)\n",
      "Loading `artful-affinity-476513-t7.complete_db.ai_governance_infrastructure_roads` -> df_roads ...\n",
      "Loaded df_roads: shape=(299, 13)\n",
      "Loading `artful-affinity-476513-t7.complete_db.ai_governance_public_safety_reports` -> df_safety ...\n",
      "Loaded df_safety: shape=(53, 12)\n",
      "Loading `artful-affinity-476513-t7.complete_db.ai_governance_citizen_services_requests` -> df_services ...\n",
      "Loaded df_services: shape=(93, 13)\n",
      "Loading `artful-affinity-476513-t7.complete_db.ai_governance_environment_monitoring` -> df_env ...\n",
      "Loaded df_env: shape=(72, 16)\n",
      "Loading `artful-affinity-476513-t7.complete_db.ai_governance_agriculture_insights` -> df_agri ...\n",
      "Loaded df_agri: shape=(78, 16)\n",
      "Loading `artful-affinity-476513-t7.complete_db.ai_governance_population_demographics` -> df_pop ...\n",
      "Loaded df_pop: shape=(100, 16)\n",
      "\n",
      "df_health sample (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospital_name</th>\n",
       "      <th>district</th>\n",
       "      <th>patient_inflow_daily</th>\n",
       "      <th>disease_outbreak</th>\n",
       "      <th>vaccination_coverage</th>\n",
       "      <th>staff_count</th>\n",
       "      <th>hospital_type</th>\n",
       "      <th>emergency_services</th>\n",
       "      <th>report_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apex Hospital</td>\n",
       "      <td>Gondia</td>\n",
       "      <td>100</td>\n",
       "      <td>None</td>\n",
       "      <td>69</td>\n",
       "      <td>60</td>\n",
       "      <td>Private</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-11-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Satara District Hospital</td>\n",
       "      <td>Satara</td>\n",
       "      <td>160</td>\n",
       "      <td>None</td>\n",
       "      <td>78</td>\n",
       "      <td>110</td>\n",
       "      <td>Public</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-07-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lifeline Hospital</td>\n",
       "      <td>Aurangabad</td>\n",
       "      <td>170</td>\n",
       "      <td>None</td>\n",
       "      <td>80</td>\n",
       "      <td>130</td>\n",
       "      <td>Public</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-09-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              hospital_name    district  patient_inflow_daily  \\\n",
       "0             Apex Hospital      Gondia                   100   \n",
       "1  Satara District Hospital      Satara                   160   \n",
       "2         Lifeline Hospital  Aurangabad                   170   \n",
       "\n",
       "  disease_outbreak  vaccination_coverage  staff_count hospital_type  \\\n",
       "0             None                    69           60       Private   \n",
       "1             None                    78          110        Public   \n",
       "2             None                    80          130        Public   \n",
       "\n",
       "   emergency_services report_date  \n",
       "0                True  2024-11-09  \n",
       "1                True  2025-07-25  \n",
       "2                True  2025-09-23  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_roads sample (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>road_id</th>\n",
       "      <th>district</th>\n",
       "      <th>road_name</th>\n",
       "      <th>construction_year</th>\n",
       "      <th>last_maintenance_year</th>\n",
       "      <th>condition_score</th>\n",
       "      <th>repair_requests_last_quarter</th>\n",
       "      <th>estimated_repair_cost</th>\n",
       "      <th>status</th>\n",
       "      <th>road_length_km</th>\n",
       "      <th>traffic_volume_daily</th>\n",
       "      <th>surface_type</th>\n",
       "      <th>last_maintenance_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROAD-035</td>\n",
       "      <td>Mumbai Suburban</td>\n",
       "      <td>SV Road</td>\n",
       "      <td>1990</td>\n",
       "      <td>2024</td>\n",
       "      <td>8.2</td>\n",
       "      <td>3</td>\n",
       "      <td>600000</td>\n",
       "      <td>Good</td>\n",
       "      <td>15.0</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>Concrete</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROAD-150</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Chakan Shikrapur Road</td>\n",
       "      <td>2003</td>\n",
       "      <td>2024</td>\n",
       "      <td>8.2</td>\n",
       "      <td>3</td>\n",
       "      <td>600000</td>\n",
       "      <td>Good</td>\n",
       "      <td>25.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>Concrete</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ROAD-243</td>\n",
       "      <td>Mumbai Suburban</td>\n",
       "      <td>Bandra-Worli Sea Link</td>\n",
       "      <td>2009</td>\n",
       "      <td>2024</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3</td>\n",
       "      <td>500000</td>\n",
       "      <td>Good</td>\n",
       "      <td>6.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>Concrete</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    road_id         district              road_name  construction_year  \\\n",
       "0  ROAD-035  Mumbai Suburban                SV Road               1990   \n",
       "1  ROAD-150             Pune  Chakan Shikrapur Road               2003   \n",
       "2  ROAD-243  Mumbai Suburban  Bandra-Worli Sea Link               2009   \n",
       "\n",
       "   last_maintenance_year  condition_score  repair_requests_last_quarter  \\\n",
       "0                   2024              8.2                             3   \n",
       "1                   2024              8.2                             3   \n",
       "2                   2024              9.0                             3   \n",
       "\n",
       "   estimated_repair_cost status  road_length_km  traffic_volume_daily  \\\n",
       "0                 600000   Good            15.0               80000.0   \n",
       "1                 600000   Good            25.0               40000.0   \n",
       "2                 500000   Good             6.0               90000.0   \n",
       "\n",
       "  surface_type last_maintenance_date  \n",
       "0     Concrete            2024-01-01  \n",
       "1     Concrete            2024-01-01  \n",
       "2     Concrete            2024-01-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_safety sample (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>police_station</th>\n",
       "      <th>district</th>\n",
       "      <th>complaints_logged</th>\n",
       "      <th>crime_reports</th>\n",
       "      <th>avg_response_time_minutes</th>\n",
       "      <th>resolved_cases_percentage</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>station_type</th>\n",
       "      <th>officers_on_duty</th>\n",
       "      <th>community_engagement_score</th>\n",
       "      <th>priority_cases_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PS-028</td>\n",
       "      <td>Gadchiroli Police Station</td>\n",
       "      <td>Gadchiroli</td>\n",
       "      <td>55</td>\n",
       "      <td>[{\"type\": \"Theft\", \"count\": 12}, {\"type\": \"Ass...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>Rural</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PS-030</td>\n",
       "      <td>Hingoli City Police Station</td>\n",
       "      <td>Hingoli</td>\n",
       "      <td>60</td>\n",
       "      <td>[{\"type\": \"Theft\", \"count\": 13}, {\"type\": \"Ass...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>61.5</td>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>Rural</td>\n",
       "      <td>27</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PS-023</td>\n",
       "      <td>Sawantwadi Police Station</td>\n",
       "      <td>Sindhudurg</td>\n",
       "      <td>65</td>\n",
       "      <td>[{\"type\": \"Theft\", \"count\": 14}, {\"type\": \"Ass...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>Rural</td>\n",
       "      <td>28</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id               police_station    district  complaints_logged  \\\n",
       "0     PS-028    Gadchiroli Police Station  Gadchiroli                 55   \n",
       "1     PS-030  Hingoli City Police Station     Hingoli                 60   \n",
       "2     PS-023    Sawantwadi Police Station  Sindhudurg                 65   \n",
       "\n",
       "                                       crime_reports  \\\n",
       "0  [{\"type\": \"Theft\", \"count\": 12}, {\"type\": \"Ass...   \n",
       "1  [{\"type\": \"Theft\", \"count\": 13}, {\"type\": \"Ass...   \n",
       "2  [{\"type\": \"Theft\", \"count\": 14}, {\"type\": \"Ass...   \n",
       "\n",
       "   avg_response_time_minutes  resolved_cases_percentage last_updated  \\\n",
       "0                       40.0                       55.0   2025-10-31   \n",
       "1                       39.0                       61.5   2025-10-31   \n",
       "2                       38.0                       60.0   2025-10-31   \n",
       "\n",
       "  station_type  officers_on_duty  community_engagement_score  \\\n",
       "0        Rural                25                         5.0   \n",
       "1        Rural                27                         5.5   \n",
       "2        Rural                28                         5.6   \n",
       "\n",
       "   priority_cases_count  \n",
       "0                   2.0  \n",
       "1                   3.0  \n",
       "2                   3.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_services sample (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>district</th>\n",
       "      <th>service_type</th>\n",
       "      <th>request_date</th>\n",
       "      <th>resolution_date</th>\n",
       "      <th>response_time_hours</th>\n",
       "      <th>satisfaction_score</th>\n",
       "      <th>status</th>\n",
       "      <th>feedback_comments</th>\n",
       "      <th>citizen_name</th>\n",
       "      <th>severity</th>\n",
       "      <th>department_assigned</th>\n",
       "      <th>closure_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REQ-093</td>\n",
       "      <td>Beed</td>\n",
       "      <td>Electricity Outage</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>Power outage was short</td>\n",
       "      <td>Var</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REQ-004</td>\n",
       "      <td>Nashik</td>\n",
       "      <td>Electricity Outage</td>\n",
       "      <td>2025-10-28</td>\n",
       "      <td>2025-10-28</td>\n",
       "      <td>6.2</td>\n",
       "      <td>8.8</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>Power restored quickly</td>\n",
       "      <td>Vivek Mehta</td>\n",
       "      <td>High</td>\n",
       "      <td>Electricity Board</td>\n",
       "      <td>Power grid fixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REQ-011</td>\n",
       "      <td>Jalgaon</td>\n",
       "      <td>Electricity Outage</td>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>4.1</td>\n",
       "      <td>9.2</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>Very quick service</td>\n",
       "      <td>Priyanka Raut</td>\n",
       "      <td>High</td>\n",
       "      <td>Electricity Board</td>\n",
       "      <td>Power grid fixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id district        service_type request_date resolution_date  \\\n",
       "0    REQ-093     Beed  Electricity Outage   2025-10-23      2025-10-23   \n",
       "1    REQ-004   Nashik  Electricity Outage   2025-10-28      2025-10-28   \n",
       "2    REQ-011  Jalgaon  Electricity Outage   2025-10-31      2025-10-31   \n",
       "\n",
       "   response_time_hours  satisfaction_score    status       feedback_comments  \\\n",
       "0                  4.0                 9.1  Resolved  Power outage was short   \n",
       "1                  6.2                 8.8  Resolved  Power restored quickly   \n",
       "2                  4.1                 9.2  Resolved      Very quick service   \n",
       "\n",
       "    citizen_name severity department_assigned    closure_reason  \n",
       "0            Var     None                None              None  \n",
       "1    Vivek Mehta     High   Electricity Board  Power grid fixed  \n",
       "2  Priyanka Raut     High   Electricity Board  Power grid fixed  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_env sample (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>district</th>\n",
       "      <th>location_name</th>\n",
       "      <th>air_quality_index</th>\n",
       "      <th>pm25_level</th>\n",
       "      <th>pm10_level</th>\n",
       "      <th>co_level</th>\n",
       "      <th>noise_level_db</th>\n",
       "      <th>waste_collection_efficiency</th>\n",
       "      <th>water_quality_index</th>\n",
       "      <th>last_inspection_date</th>\n",
       "      <th>status</th>\n",
       "      <th>monitoring_type</th>\n",
       "      <th>dominant_pollutant</th>\n",
       "      <th>vegetation_cover_percentage</th>\n",
       "      <th>renewable_energy_usage_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENV-061</td>\n",
       "      <td>Gadchiroli</td>\n",
       "      <td>Etapalli Region</td>\n",
       "      <td>42.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>Good</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>87.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENV-028</td>\n",
       "      <td>Gadchiroli</td>\n",
       "      <td>Desaiganj Forest</td>\n",
       "      <td>45.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>Good</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENV-058</td>\n",
       "      <td>Wardha</td>\n",
       "      <td>Samudrapur Rural</td>\n",
       "      <td>48.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2025-10-30</td>\n",
       "      <td>Good</td>\n",
       "      <td>All</td>\n",
       "      <td>None</td>\n",
       "      <td>82.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id    district     location_name  air_quality_index  pm25_level  \\\n",
       "0    ENV-061  Gadchiroli   Etapalli Region               42.0        11.0   \n",
       "1    ENV-028  Gadchiroli  Desaiganj Forest               45.0        12.0   \n",
       "2    ENV-058      Wardha  Samudrapur Rural               48.0        14.0   \n",
       "\n",
       "   pm10_level  co_level  noise_level_db  waste_collection_efficiency  \\\n",
       "0        24.0       0.1            38.0                         99.0   \n",
       "1        25.0       0.1            40.0                         98.0   \n",
       "2        28.0       0.1            45.0                         99.0   \n",
       "\n",
       "   water_quality_index last_inspection_date status monitoring_type  \\\n",
       "0                 99.0           2025-10-29   Good           Water   \n",
       "1                 98.0           2025-10-29   Good           Water   \n",
       "2                 97.0           2025-10-30   Good             All   \n",
       "\n",
       "  dominant_pollutant  vegetation_cover_percentage  \\\n",
       "0               None                         87.0   \n",
       "1               None                         85.0   \n",
       "2               None                         82.0   \n",
       "\n",
       "   renewable_energy_usage_percent  \n",
       "0                            52.0  \n",
       "1                            50.0  \n",
       "2                            48.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_agri sample (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>farm_id</th>\n",
       "      <th>district</th>\n",
       "      <th>crop_type</th>\n",
       "      <th>farm_area_hectares</th>\n",
       "      <th>soil_quality_score</th>\n",
       "      <th>average_yield_tonnes</th>\n",
       "      <th>irrigation_source</th>\n",
       "      <th>rainfall_mm_last_month</th>\n",
       "      <th>fertilizer_usage_kg_per_hectare</th>\n",
       "      <th>pest_incidents_last_season</th>\n",
       "      <th>crop_health_index</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>farmer_name</th>\n",
       "      <th>farming_method</th>\n",
       "      <th>market_price_per_tonne_inr</th>\n",
       "      <th>crop_variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FARM-036</td>\n",
       "      <td>Nashik</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Canal</td>\n",
       "      <td>105.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>Rahul Joshi</td>\n",
       "      <td>Conventional</td>\n",
       "      <td>29000.0</td>\n",
       "      <td>Raj-3077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FARM-052</td>\n",
       "      <td>Osmanabad</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Canal</td>\n",
       "      <td>95.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2025-10-28</td>\n",
       "      <td>Sanjay Jagtap</td>\n",
       "      <td>Conventional</td>\n",
       "      <td>29500.0</td>\n",
       "      <td>HI-8663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FARM-060</td>\n",
       "      <td>Beed</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Canal</td>\n",
       "      <td>92.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>2025-10-28</td>\n",
       "      <td>Priyanka Thorat</td>\n",
       "      <td>Conventional</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>NI-5439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    farm_id   district crop_type  farm_area_hectares  soil_quality_score  \\\n",
       "0  FARM-036     Nashik     Wheat                 5.0                 7.0   \n",
       "1  FARM-052  Osmanabad     Wheat                 5.5                 6.8   \n",
       "2  FARM-060       Beed     Wheat                 6.5                 6.9   \n",
       "\n",
       "   average_yield_tonnes irrigation_source  rainfall_mm_last_month  \\\n",
       "0                   3.8             Canal                   105.0   \n",
       "1                   3.9             Canal                    95.0   \n",
       "2                   4.0             Canal                    92.0   \n",
       "\n",
       "   fertilizer_usage_kg_per_hectare  pest_incidents_last_season  \\\n",
       "0                             80.0                           2   \n",
       "1                             84.0                           2   \n",
       "2                             86.0                           2   \n",
       "\n",
       "   crop_health_index last_updated      farmer_name farming_method  \\\n",
       "0                7.7   2025-10-29      Rahul Joshi   Conventional   \n",
       "1                7.7   2025-10-28    Sanjay Jagtap   Conventional   \n",
       "2                7.8   2025-10-28  Priyanka Thorat   Conventional   \n",
       "\n",
       "   market_price_per_tonne_inr crop_variety  \n",
       "0                     29000.0     Raj-3077  \n",
       "1                     29500.0      HI-8663  \n",
       "2                     30000.0      NI-5439  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_pop sample (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district_id</th>\n",
       "      <th>district</th>\n",
       "      <th>total_population</th>\n",
       "      <th>urban_population_percentage</th>\n",
       "      <th>rural_population_percentage</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>literacy_rate</th>\n",
       "      <th>employment_rate</th>\n",
       "      <th>youth_population_percentage</th>\n",
       "      <th>senior_population_percentage</th>\n",
       "      <th>avg_household_size</th>\n",
       "      <th>population_density_per_sqkm</th>\n",
       "      <th>last_census_year</th>\n",
       "      <th>projected_growth_rate</th>\n",
       "      <th>last_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DIST-075</td>\n",
       "      <td>Mangalwedha</td>\n",
       "      <td>9267296</td>\n",
       "      <td>78.4</td>\n",
       "      <td>21.6</td>\n",
       "      <td>4733369</td>\n",
       "      <td>4533927</td>\n",
       "      <td>85.7</td>\n",
       "      <td>55.6</td>\n",
       "      <td>23.9</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4617.3</td>\n",
       "      <td>2021</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2025-10-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DIST-049</td>\n",
       "      <td>Dombivli</td>\n",
       "      <td>4281658</td>\n",
       "      <td>75.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2218163</td>\n",
       "      <td>2063495</td>\n",
       "      <td>73.7</td>\n",
       "      <td>61.8</td>\n",
       "      <td>22.7</td>\n",
       "      <td>9.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6197.3</td>\n",
       "      <td>2021</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2025-10-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DIST-100</td>\n",
       "      <td>Niphad</td>\n",
       "      <td>10107905</td>\n",
       "      <td>39.2</td>\n",
       "      <td>60.8</td>\n",
       "      <td>5105491</td>\n",
       "      <td>5002414</td>\n",
       "      <td>70.9</td>\n",
       "      <td>59.5</td>\n",
       "      <td>19.7</td>\n",
       "      <td>10.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>8633.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2025-10-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  district_id     district  total_population  urban_population_percentage  \\\n",
       "0    DIST-075  Mangalwedha           9267296                         78.4   \n",
       "1    DIST-049     Dombivli           4281658                         75.0   \n",
       "2    DIST-100       Niphad          10107905                         39.2   \n",
       "\n",
       "   rural_population_percentage  male_population  female_population  \\\n",
       "0                         21.6          4733369            4533927   \n",
       "1                         25.0          2218163            2063495   \n",
       "2                         60.8          5105491            5002414   \n",
       "\n",
       "   literacy_rate  employment_rate  youth_population_percentage  \\\n",
       "0           85.7             55.6                         23.9   \n",
       "1           73.7             61.8                         22.7   \n",
       "2           70.9             59.5                         19.7   \n",
       "\n",
       "   senior_population_percentage  avg_household_size  \\\n",
       "0                           6.5                 3.8   \n",
       "1                           9.2                 3.8   \n",
       "2                          10.4                 3.9   \n",
       "\n",
       "   population_density_per_sqkm  last_census_year  projected_growth_rate  \\\n",
       "0                       4617.3              2021                    1.8   \n",
       "1                       6197.3              2021                    2.4   \n",
       "2                       8633.0              2021                    1.5   \n",
       "\n",
       "  last_updated  \n",
       "0   2025-10-28  \n",
       "1   2025-10-28  \n",
       "2   2025-10-28  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TABLE_MAP = {\n",
    "    'df_health': 'ai_governance_health_facilities',\n",
    "    'df_roads': 'ai_governance_infrastructure_roads',\n",
    "    'df_safety': 'ai_governance_public_safety_reports',\n",
    "    'df_services': 'ai_governance_citizen_services_requests',\n",
    "    'df_env': 'ai_governance_environment_monitoring',\n",
    "    'df_agri': 'ai_governance_agriculture_insights',\n",
    "    'df_pop': 'ai_governance_population_demographics',\n",
    "}\n",
    "\n",
    "# Optional: limit rows for quick iteration (set to None to load full table)\n",
    "ROW_LIMIT = None  # e.g., 20000 or None\n",
    "\n",
    "for varname, table in TABLE_MAP.items():\n",
    "    fq_table = f\"{PROJECT_ID}.{BQ_DATASET}.{table}\"\n",
    "    try:\n",
    "        print(f\"Loading `{fq_table}` -> {varname} ...\")\n",
    "        if ROW_LIMIT:\n",
    "            sql = f\"SELECT * FROM `{fq_table}` LIMIT {ROW_LIMIT}\"\n",
    "        else:\n",
    "            sql = f\"SELECT * FROM `{fq_table}`\"\n",
    "        job = bq_client.query(sql)\n",
    "        df = job.to_dataframe()   # may take time for big tables\n",
    "        globals()[varname] = df\n",
    "        print(f\"Loaded {varname}: shape={df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load `{fq_table}` into {varname}: {e}\")\n",
    "\n",
    "normalize_tables()\n",
    "print('Applied normalization to loaded tables (district_norm + datetime coercion).')\n",
    "\n",
    "# Quick peek\n",
    "for var in TABLE_MAP.keys():\n",
    "    if var in globals() and getattr(globals()[var], \"shape\", (0,0))[0] > 0:\n",
    "        print(f\"\\n{var} sample (first 3 rows):\")\n",
    "        display(globals()[var].head(3))\n",
    "    else:\n",
    "        print(f\"\\n{var} is empty or not found (shape={getattr(globals().get(var), 'shape', None)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe05b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred date columns: {'health': 'report_date', 'roads': 'last_maintenance_date', 'safety': 'last_updated', 'services': 'request_date', 'env': 'last_inspection_date', 'agri': 'last_updated'}\n",
      "Panel date range: 2024-01-03 00:00:00 to 2025-11-02 00:00:00\n",
      "Health weekly rows: 215\n",
      "Weeks with outbreak_count>0: 110\n",
      "patient_inflow_mean coverage: 1.0\n",
      "Panel shape: (9500, 34)\n",
      "     district week_start  patient_inflow_mean  outbreak_count  health_events  \\\n",
      "0  Ahmednagar 2024-01-08                 <NA>             0.0            0.0   \n",
      "1  Ahmednagar 2024-01-15                 <NA>             0.0            0.0   \n",
      "2  Ahmednagar 2024-01-22                 <NA>             0.0            0.0   \n",
      "3  Ahmednagar 2024-01-29                 <NA>             0.0            0.0   \n",
      "4  Ahmednagar 2024-02-05                 <NA>             0.0            0.0   \n",
      "\n",
      "   air_quality_index  pm25_level  pm10_level  water_quality_index  \\\n",
      "0                NaN         NaN         NaN                  NaN   \n",
      "1                NaN         NaN         NaN                  NaN   \n",
      "2                NaN         NaN         NaN                  NaN   \n",
      "3                NaN         NaN         NaN                  NaN   \n",
      "4                NaN         NaN         NaN                  NaN   \n",
      "\n",
      "   waste_collection_efficiency  ...  crime_Vandalism  total_population  \\\n",
      "0                          NaN  ...              0.0           4115207   \n",
      "1                          NaN  ...              0.0           4115207   \n",
      "2                          NaN  ...              0.0           4115207   \n",
      "3                          NaN  ...              0.0           4115207   \n",
      "4                          NaN  ...              0.0           4115207   \n",
      "\n",
      "   pop_per_100k  pm25_roll_1w  patient_inflow_roll_1w  pm25_roll_2w  \\\n",
      "0      41.15207           NaN                     NaN           NaN   \n",
      "1      41.15207           NaN                     NaN           NaN   \n",
      "2      41.15207           NaN                     NaN           NaN   \n",
      "3      41.15207           NaN                     NaN           NaN   \n",
      "4      41.15207           NaN                     NaN           NaN   \n",
      "\n",
      "   patient_inflow_roll_2w  pm25_roll_4w  patient_inflow_roll_4w  \\\n",
      "0                     NaN           NaN                     NaN   \n",
      "1                     NaN           NaN                     NaN   \n",
      "2                     NaN           NaN                     NaN   \n",
      "3                     NaN           NaN                     NaN   \n",
      "4                     NaN           NaN                     NaN   \n",
      "\n",
      "   outbreak_next_14d  \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  0  \n",
      "4                  0  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "Saved CLEAN district_week_panel_demo.csv (no rolling warnings).\n"
     ]
    }
   ],
   "source": [
    "def first_existing(df, candidates):\n",
    "    for column in candidates:\n",
    "        if column in df.columns:\n",
    "            return column\n",
    "    return None\n",
    "\n",
    "def parse_crime_reports(value):\n",
    "    if pd.isna(value):\n",
    "        return {}\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            parsed = json.loads(value)\n",
    "        except Exception:\n",
    "            return {}\n",
    "    elif isinstance(value, dict):\n",
    "        parsed = [value]\n",
    "    else:\n",
    "        parsed = value\n",
    "    if not isinstance(parsed, (list, tuple)):\n",
    "        return {}\n",
    "    counts = {}\n",
    "    for item in parsed:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "        crime_type = item.get('type')\n",
    "        count = item.get('count', 1)\n",
    "        try:\n",
    "            count = int(float(count))\n",
    "        except Exception:\n",
    "            count = 0\n",
    "        if crime_type and count:\n",
    "            counts[crime_type] = counts.get(crime_type, 0) + count\n",
    "    return counts\n",
    "\n",
    "for suffix in ['health', 'env', 'safety', 'services', 'roads', 'agri', 'pop']:\n",
    "    var_name = f'df_{suffix}'\n",
    "    if var_name not in globals():\n",
    "        globals()[var_name] = pd.DataFrame()\n",
    "\n",
    "date_cols = {}\n",
    "for suffix in ['health', 'env', 'safety', 'services', 'roads', 'agri']:\n",
    "    df = globals()[f'df_{suffix}']\n",
    "    if not df.empty and 'district_norm' not in df.columns and 'district' in df.columns:\n",
    "        df['district_norm'] = norm_dist(df['district'])\n",
    "    date_col = find_date_col(df)\n",
    "    date_cols[suffix] = date_col\n",
    "    if date_col:\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    globals()[f'df_{suffix}'] = df\n",
    "\n",
    "if not df_pop.empty and 'district_norm' not in df_pop.columns and 'district' in df_pop.columns:\n",
    "    df_pop['district_norm'] = norm_dist(df_pop['district'])\n",
    "\n",
    "# Health weekly aggregation\n",
    "health_week = pd.DataFrame()\n",
    "if not df_health.empty and date_cols.get('health'):\n",
    "    health_date = date_cols['health']\n",
    "    patient_col = first_existing(df_health, ['patient_inflow_daily', 'patient_inflow', 'patient_inflow_mean'])\n",
    "    outbreak_col = first_existing(df_health, ['disease_outbreak'])\n",
    "    health_cols = ['district_norm', health_date]\n",
    "    if patient_col:\n",
    "        health_cols.append(patient_col)\n",
    "    if outbreak_col:\n",
    "        health_cols.append(outbreak_col)\n",
    "    h = df_health[health_cols].copy()\n",
    "    h = h.dropna(subset=['district_norm'])\n",
    "    h['week_start'] = norm_week(h[health_date])\n",
    "    h = h.dropna(subset=['week_start'])\n",
    "    h['health_events'] = 1\n",
    "    agg_map = {'health_events': 'sum'}\n",
    "    if patient_col:\n",
    "        h['patient_inflow_value'] = pd.to_numeric(h[patient_col], errors='coerce')\n",
    "        agg_map['patient_inflow_value'] = 'mean'\n",
    "    if outbreak_col:\n",
    "        outbreak_series = h[outbreak_col].astype(str).str.strip().str.lower()\n",
    "        h['outbreak_flag'] = (~outbreak_series.isin({'', 'none', 'null', 'nan'})).astype(int)\n",
    "        agg_map['outbreak_flag'] = 'sum'\n",
    "    health_week = h.groupby(['district_norm', 'week_start']).agg(agg_map).reset_index()\n",
    "    if 'patient_inflow_value' in health_week.columns:\n",
    "        health_week = health_week.rename(columns={'patient_inflow_value': 'patient_inflow_mean'})\n",
    "    if 'outbreak_flag' in health_week.columns:\n",
    "        health_week = health_week.rename(columns={'outbreak_flag': 'outbreak_count'})\n",
    "    else:\n",
    "        health_week['outbreak_count'] = 0\n",
    "    if 'health_events' not in health_week.columns:\n",
    "        health_week['health_events'] = 0\n",
    "\n",
    "# Environment weekly aggregation\n",
    "env_week = pd.DataFrame()\n",
    "if not df_env.empty and date_cols.get('env'):\n",
    "    env_date = date_cols['env']\n",
    "    env_metrics = [c for c in ['air_quality_index', 'pm25_level', 'pm10_level', 'water_quality_index', 'waste_collection_efficiency'] if c in df_env.columns]\n",
    "    if env_metrics:\n",
    "        env_cols = ['district_norm', env_date] + env_metrics\n",
    "        e = df_env[env_cols].copy()\n",
    "        e = e.dropna(subset=['district_norm'])\n",
    "        e['week_start'] = norm_week(e[env_date])\n",
    "        e = e.dropna(subset=['week_start'])\n",
    "        agg_map = {c: 'mean' for c in env_metrics}\n",
    "        env_week = e.groupby(['district_norm', 'week_start']).agg(agg_map).reset_index()\n",
    "\n",
    "# Services weekly aggregation\n",
    "services_week = pd.DataFrame()\n",
    "if not df_services.empty and date_cols.get('services'):\n",
    "    services_date = date_cols['services']\n",
    "    service_cols = ['district_norm', services_date]\n",
    "    if 'service_type' in df_services.columns:\n",
    "        service_cols.append('service_type')\n",
    "    svc = df_services[service_cols].copy()\n",
    "    svc = svc.dropna(subset=['district_norm'])\n",
    "    svc['week_start'] = norm_week(svc[services_date])\n",
    "    svc = svc.dropna(subset=['week_start'])\n",
    "    svc['services_events'] = 1\n",
    "    agg_map = {'services_events': 'sum'}\n",
    "    if 'service_type' in svc.columns:\n",
    "        agg_map['service_type'] = 'count'\n",
    "    services_week = svc.groupby(['district_norm', 'week_start']).agg(agg_map).reset_index()\n",
    "    if 'service_type' in services_week.columns:\n",
    "        services_week = services_week.rename(columns={'service_type': 'complaint_count'})\n",
    "\n",
    "# Safety weekly aggregation from crime JSON\n",
    "safety_week = pd.DataFrame()\n",
    "if not df_safety.empty and date_cols.get('safety') and 'crime_reports' in df_safety.columns:\n",
    "    safety_date = date_cols['safety']\n",
    "    safe = df_safety[['district_norm', safety_date, 'crime_reports']].copy()\n",
    "    safe = safe.dropna(subset=['district_norm'])\n",
    "    safe['week_start'] = norm_week(safe[safety_date])\n",
    "    safe = safe.dropna(subset=['week_start'])\n",
    "    safe['crime_counts'] = safe['crime_reports'].apply(parse_crime_reports)\n",
    "    crime_rows = []\n",
    "    for _, row in safe.iterrows():\n",
    "        counts = row['crime_counts'] or {}\n",
    "        for crime_type, count in counts.items():\n",
    "            crime_rows.append({'district_norm': row['district_norm'], 'week_start': row['week_start'], 'crime_type': crime_type, 'count': count})\n",
    "    if crime_rows:\n",
    "        crime_df = pd.DataFrame(crime_rows)\n",
    "        safety_week = (\n",
    "            crime_df.groupby(['district_norm', 'week_start', 'crime_type'])['count']\n",
    "            .sum()\n",
    "            .unstack(fill_value=0)\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "for label, wk in [('health', health_week), ('env', env_week), ('services', services_week), ('safety', safety_week)]:\n",
    "    if wk is None or wk.empty:\n",
    "        print(f'{label}_week: empty')\n",
    "    else:\n",
    "        print(\n",
    "            f\"{label}_week rows={len(wk)}, districts={wk['district_norm'].nunique()}, \"\n",
    "            f\"weeks={wk['week_start'].min()}->{wk['week_start'].max()}\"\n",
    "        )\n",
    "\n",
    "weekly_tables = [health_week, env_week, services_week, safety_week]\n",
    "signal_districts = set()\n",
    "for wk in weekly_tables:\n",
    "    if wk is not None and not wk.empty:\n",
    "        signal_districts.update(wk['district_norm'].dropna().unique())\n",
    "\n",
    "if df_pop.empty:\n",
    "    raise ValueError('Population table is required to build the panel.')\n",
    "\n",
    "if 'district_norm' not in df_pop.columns and 'district' in df_pop.columns:\n",
    "    df_pop['district_norm'] = norm_dist(df_pop['district'])\n",
    "\n",
    "pop_districts = set(df_pop['district_norm'].dropna().unique())\n",
    "missing_in_pop = sorted(signal_districts - pop_districts)\n",
    "if missing_in_pop:\n",
    "    preview = ', '.join(missing_in_pop[:8])\n",
    "    print(f'Districts with weekly signal missing in population ({len(missing_in_pop)}): {preview}')\n",
    "missing_signal = sorted(pop_districts - signal_districts)\n",
    "print(f'Districts with population but no weekly signal: {len(missing_signal)}')\n",
    "\n",
    "districts = sorted(signal_districts & pop_districts)\n",
    "if not districts:\n",
    "    raise ValueError('No overlapping districts between signal tables and population table after normalization.')\n",
    "\n",
    "week_bounds = []\n",
    "for wk in weekly_tables:\n",
    "    if wk is not None and not wk.empty:\n",
    "        week_bounds.append((wk['week_start'].min(), wk['week_start'].max()))\n",
    "if not week_bounds:\n",
    "    raise ValueError('No weekly aggregates available to build panel.')\n",
    "\n",
    "min_date = min(bound[0] for bound in week_bounds)\n",
    "max_date = max(bound[1] for bound in week_bounds)\n",
    "week_starts = pd.date_range(start=min_date, end=max_date, freq='W-MON')\n",
    "week_starts = pd.Series(week_starts).dt.floor('D')\n",
    "\n",
    "panel = pd.MultiIndex.from_product([districts, week_starts], names=['district_norm', 'week_start']).to_frame(index=False)\n",
    "\n",
    "def merge_weekly(base, weekly):\n",
    "    if weekly is None or weekly.empty:\n",
    "        return base\n",
    "    weekly = weekly.copy()\n",
    "    weekly['week_start'] = pd.to_datetime(weekly['week_start']).dt.floor('D')\n",
    "    return base.merge(weekly, on=['district_norm', 'week_start'], how='left')\n",
    "\n",
    "panel['week_start'] = pd.to_datetime(panel['week_start']).dt.floor('D')\n",
    "panel = merge_weekly(panel, health_week)\n",
    "panel = merge_weekly(panel, env_week)\n",
    "panel = merge_weekly(panel, services_week)\n",
    "panel = merge_weekly(panel, safety_week)\n",
    "\n",
    "pop_cols = [c for c in ['district_norm', 'district', 'total_population', 'population_density_per_sqkm', 'avg_household_size'] if c in df_pop.columns]\n",
    "pop_frame = df_pop[pop_cols].drop_duplicates(subset=['district_norm'])\n",
    "panel = panel.merge(pop_frame, on='district_norm', how='left')\n",
    "\n",
    "if 'district' not in panel.columns:\n",
    "    panel['district'] = panel['district_norm']\n",
    "\n",
    "if 'total_population' in panel.columns:\n",
    "    panel['total_population'] = pd.to_numeric(panel['total_population'], errors='coerce')\n",
    "    panel['pop_per_100k'] = panel['total_population'].replace({0: np.nan}) / 100000.0\n",
    "else:\n",
    "    panel['pop_per_100k'] = np.nan\n",
    "\n",
    "for col in ['outbreak_count', 'health_events', 'services_events', 'complaint_count']:\n",
    "    if col in panel.columns:\n",
    "        panel[col] = pd.to_numeric(panel[col], errors='coerce').fillna(0)\n",
    "\n",
    "crime_cols = [c for c in panel.columns if c.startswith('crime_')]\n",
    "for col in crime_cols:\n",
    "    panel[col] = pd.to_numeric(panel[col], errors='coerce').fillna(0)\n",
    "    panel[col] = np.where(panel['pop_per_100k'] > 0, panel[col] / panel['pop_per_100k'], 0)\n",
    "\n",
    "cont_candidates = [\n",
    "    'patient_inflow_mean',\n",
    "    'air_quality_index',\n",
    "    'pm25_level',\n",
    "    'pm10_level',\n",
    "    'water_quality_index',\n",
    "    'waste_collection_efficiency',\n",
    "    'traffic_volume_daily',\n",
    "    'condition_score'\n",
    " ]\n",
    "cont_cols = [c for c in cont_candidates if c in panel.columns]\n",
    "dropped_cont_cols = []\n",
    "for col in cont_cols:\n",
    "    panel[col] = pd.to_numeric(panel[col], errors='coerce')\n",
    "    panel[col] = panel.groupby('district_norm')[col].transform(lambda s: s.ffill().bfill())\n",
    "    if panel[col].notna().sum() == 0:\n",
    "        panel.drop(columns=[col], inplace=True)\n",
    "        dropped_cont_cols.append(col)\n",
    "        continue\n",
    "    median_val = panel[col].median(skipna=True)\n",
    "    if pd.isna(median_val):\n",
    "        panel.drop(columns=[col], inplace=True)\n",
    "        dropped_cont_cols.append(col)\n",
    "        continue\n",
    "    panel[col] = panel[col].fillna(median_val)\n",
    "\n",
    "if dropped_cont_cols:\n",
    "    print(f'Dropped continuous columns with no usable coverage: {dropped_cont_cols}')\n",
    "\n",
    "for window in [1, 2, 4]:\n",
    "    if 'pm25_level' in panel.columns:\n",
    "        panel[f'pm25_roll_{window}w'] = panel.groupby('district_norm')['pm25_level'].transform(lambda s: s.shift(1).rolling(window, min_periods=1).mean())\n",
    "    if 'patient_inflow_mean' in panel.columns:\n",
    "        panel[f'patient_inflow_roll_{window}w'] = panel.groupby('district_norm')['patient_inflow_mean'].transform(lambda s: s.shift(1).rolling(window, min_periods=1).mean())\n",
    "\n",
    "def future_sum(series, horizon):\n",
    "    values = series.fillna(0).to_numpy()\n",
    "    out = np.zeros(len(values), dtype=float)\n",
    "    for idx in range(len(values)):\n",
    "        start = idx + 1\n",
    "        end = min(idx + 1 + horizon, len(values))\n",
    "        out[idx] = values[start:end].sum()\n",
    "    return pd.Series(out, index=series.index)\n",
    "\n",
    "if 'outbreak_count' not in panel.columns:\n",
    "    panel['outbreak_count'] = 0\n",
    "panel['outbreak_count'] = pd.to_numeric(panel['outbreak_count'], errors='coerce').fillna(0)\n",
    "\n",
    "HORIZON_WEEKS = 2\n",
    "panel = panel.sort_values(['district_norm', 'week_start']).reset_index(drop=True)\n",
    "panel['outbreak_next_14d'] = (\n",
    "    panel.groupby('district_norm')['outbreak_count']\n",
    "         .transform(lambda s: future_sum(s, HORIZON_WEEKS))\n",
    "         .gt(0)\n",
    "         .astype(int)\n",
    " )\n",
    "\n",
    "def inner_hits(base, weekly):\n",
    "    if weekly is None or weekly.empty:\n",
    "        return 0\n",
    "    pairs = weekly[['district_norm', 'week_start']].dropna().drop_duplicates()\n",
    "    pairs['week_start'] = pd.to_datetime(pairs['week_start']).dt.floor('D')\n",
    "    return (\n",
    "        base[['district_norm', 'week_start']]\n",
    "        .merge(pairs, on=['district_norm', 'week_start'], how='inner')\n",
    "        .shape[0]\n",
    "    )\n",
    "\n",
    "print('Panel districts:', len(districts))\n",
    "print('Panel weeks:', len(week_starts))\n",
    "print('Rows with outbreak_count>0:', int((panel['outbreak_count'] > 0).sum()))\n",
    "print('Positive rate (outbreak_next_14d):', panel['outbreak_next_14d'].mean())\n",
    "print('Inner hits with health_week:', inner_hits(panel, health_week))\n",
    "print('Inner hits with env_week:', inner_hits(panel, env_week))\n",
    "print('Inner hits with services_week:', inner_hits(panel, services_week))\n",
    "print('Inner hits with safety_week:', inner_hits(panel, safety_week))\n",
    "if not env_week.empty:\n",
    "    env_share = (\n",
    "        panel[['district_norm', 'week_start']]\n",
    "        .merge(env_week[['district_norm', 'week_start']].drop_duplicates(), how='inner')\n",
    "        .shape[0] / len(panel)\n",
    "    )\n",
    "    print(f'Share of rows with raw env coverage: {env_share:.3f}')\n",
    "else:\n",
    "    print('Share of rows with raw env coverage: 0.000')\n",
    "\n",
    "df_model = panel.copy()\n",
    "df_model.to_csv('district_week_panel_demo.csv', index=False)\n",
    "print('Saved CLEAN district_week_panel_demo.csv (normalized panel).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50c11dbc-450e-4ef9-8acd-edf4ebb1ddac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop districts (sample): ['Mangalwedha' 'Dombivli' 'Niphad' 'Chakan' 'Yeola' 'Badlapur' 'Panhala'\n",
      " 'Kalwan' 'Ambegaon' 'Dodamarg']\n",
      "health districts (sample): ['Gondia' 'Satara' 'Aurangabad' 'Ahmednagar' 'Sindhudurg' 'Parbhani'\n",
      " 'Latur' 'Nanded' 'Dhule' 'Nashik']\n",
      "overlap count: 31\n"
     ]
    }
   ],
   "source": [
    "# 1) Compare district vocabularies\n",
    "print(\"pop districts (sample):\", df_pop['district'].dropna().unique()[:10])\n",
    "print(\"health districts (sample):\", df_health['district'].dropna().unique()[:10])\n",
    "\n",
    "# 2) How many overlaps?\n",
    "pop_set = set(df_pop['district'].dropna().str.strip().str.lower())\n",
    "health_set = set(df_health['district'].dropna().str.strip().str.lower())\n",
    "print(\"overlap count:\", len(pop_set & health_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f9691a3-e341-4bf7-bd0f-f739bd082ead",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_health columns: ['hospital_name', 'district', 'patient_inflow_daily', 'disease_outbreak', 'vaccination_coverage', 'staff_count', 'hospital_type', 'emergency_services', 'report_date']\n",
      "hospital_name                   object\n",
      "district                        object\n",
      "patient_inflow_daily             Int64\n",
      "disease_outbreak                object\n",
      "vaccination_coverage             Int64\n",
      "staff_count                      Int64\n",
      "hospital_type                   object\n",
      "emergency_services             boolean\n",
      "report_date             datetime64[ns]\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospital_name</th>\n",
       "      <th>district</th>\n",
       "      <th>patient_inflow_daily</th>\n",
       "      <th>disease_outbreak</th>\n",
       "      <th>vaccination_coverage</th>\n",
       "      <th>staff_count</th>\n",
       "      <th>hospital_type</th>\n",
       "      <th>emergency_services</th>\n",
       "      <th>report_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apex Hospital</td>\n",
       "      <td>Gondia</td>\n",
       "      <td>100</td>\n",
       "      <td>None</td>\n",
       "      <td>69</td>\n",
       "      <td>60</td>\n",
       "      <td>Private</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-11-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Satara District Hospital</td>\n",
       "      <td>Satara</td>\n",
       "      <td>160</td>\n",
       "      <td>None</td>\n",
       "      <td>78</td>\n",
       "      <td>110</td>\n",
       "      <td>Public</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-07-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lifeline Hospital</td>\n",
       "      <td>Aurangabad</td>\n",
       "      <td>170</td>\n",
       "      <td>None</td>\n",
       "      <td>80</td>\n",
       "      <td>130</td>\n",
       "      <td>Public</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>City Medical Center</td>\n",
       "      <td>Ahmednagar</td>\n",
       "      <td>170</td>\n",
       "      <td>None</td>\n",
       "      <td>81</td>\n",
       "      <td>100</td>\n",
       "      <td>Private</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-09-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Vision Clinic</td>\n",
       "      <td>Ahmednagar</td>\n",
       "      <td>160</td>\n",
       "      <td>None</td>\n",
       "      <td>82</td>\n",
       "      <td>90</td>\n",
       "      <td>Private</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-04-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              hospital_name    district  patient_inflow_daily  \\\n",
       "0             Apex Hospital      Gondia                   100   \n",
       "1  Satara District Hospital      Satara                   160   \n",
       "2         Lifeline Hospital  Aurangabad                   170   \n",
       "3       City Medical Center  Ahmednagar                   170   \n",
       "4         New Vision Clinic  Ahmednagar                   160   \n",
       "\n",
       "  disease_outbreak  vaccination_coverage  staff_count hospital_type  \\\n",
       "0             None                    69           60       Private   \n",
       "1             None                    78          110        Public   \n",
       "2             None                    80          130        Public   \n",
       "3             None                    81          100       Private   \n",
       "4             None                    82           90       Private   \n",
       "\n",
       "   emergency_services report_date  \n",
       "0                True  2024-11-09  \n",
       "1                True  2025-07-25  \n",
       "2                True  2025-09-23  \n",
       "3               False  2025-09-09  \n",
       "4               False  2025-04-05  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_env columns: ['station_id', 'district', 'location_name', 'air_quality_index', 'pm25_level', 'pm10_level', 'co_level', 'noise_level_db', 'waste_collection_efficiency', 'water_quality_index', 'last_inspection_date', 'status', 'monitoring_type', 'dominant_pollutant', 'vegetation_cover_percentage', 'renewable_energy_usage_percent']\n",
      "station_id                                object\n",
      "district                                  object\n",
      "location_name                             object\n",
      "air_quality_index                        float64\n",
      "pm25_level                               float64\n",
      "pm10_level                               float64\n",
      "co_level                                 float64\n",
      "noise_level_db                           float64\n",
      "waste_collection_efficiency              float64\n",
      "water_quality_index                      float64\n",
      "last_inspection_date              datetime64[ns]\n",
      "status                                    object\n",
      "monitoring_type                           object\n",
      "dominant_pollutant                        object\n",
      "vegetation_cover_percentage              float64\n",
      "renewable_energy_usage_percent           float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>district</th>\n",
       "      <th>location_name</th>\n",
       "      <th>air_quality_index</th>\n",
       "      <th>pm25_level</th>\n",
       "      <th>pm10_level</th>\n",
       "      <th>co_level</th>\n",
       "      <th>noise_level_db</th>\n",
       "      <th>waste_collection_efficiency</th>\n",
       "      <th>water_quality_index</th>\n",
       "      <th>last_inspection_date</th>\n",
       "      <th>status</th>\n",
       "      <th>monitoring_type</th>\n",
       "      <th>dominant_pollutant</th>\n",
       "      <th>vegetation_cover_percentage</th>\n",
       "      <th>renewable_energy_usage_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENV-061</td>\n",
       "      <td>Gadchiroli</td>\n",
       "      <td>Etapalli Region</td>\n",
       "      <td>42.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>Good</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>87.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENV-028</td>\n",
       "      <td>Gadchiroli</td>\n",
       "      <td>Desaiganj Forest</td>\n",
       "      <td>45.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>Good</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENV-058</td>\n",
       "      <td>Wardha</td>\n",
       "      <td>Samudrapur Rural</td>\n",
       "      <td>48.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2025-10-30</td>\n",
       "      <td>Good</td>\n",
       "      <td>All</td>\n",
       "      <td>None</td>\n",
       "      <td>82.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENV-025</td>\n",
       "      <td>Wardha</td>\n",
       "      <td>Sevagram Rural</td>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>48.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2025-10-30</td>\n",
       "      <td>Good</td>\n",
       "      <td>All</td>\n",
       "      <td>None</td>\n",
       "      <td>80.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENV-022</td>\n",
       "      <td>Ratnagiri</td>\n",
       "      <td>Ganpatipule Beach</td>\n",
       "      <td>55.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>Good</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>75.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id    district      location_name  air_quality_index  pm25_level  \\\n",
       "0    ENV-061  Gadchiroli    Etapalli Region               42.0        11.0   \n",
       "1    ENV-028  Gadchiroli   Desaiganj Forest               45.0        12.0   \n",
       "2    ENV-058      Wardha   Samudrapur Rural               48.0        14.0   \n",
       "3    ENV-025      Wardha     Sevagram Rural               50.0        15.0   \n",
       "4    ENV-022   Ratnagiri  Ganpatipule Beach               55.0        18.0   \n",
       "\n",
       "   pm10_level  co_level  noise_level_db  waste_collection_efficiency  \\\n",
       "0        24.0       0.1            38.0                         99.0   \n",
       "1        25.0       0.1            40.0                         98.0   \n",
       "2        28.0       0.1            45.0                         99.0   \n",
       "3        30.0       0.2            48.0                         97.0   \n",
       "4        35.0       0.2            50.0                         96.0   \n",
       "\n",
       "   water_quality_index last_inspection_date status monitoring_type  \\\n",
       "0                 99.0           2025-10-29   Good           Water   \n",
       "1                 98.0           2025-10-29   Good           Water   \n",
       "2                 97.0           2025-10-30   Good             All   \n",
       "3                 96.0           2025-10-30   Good             All   \n",
       "4                 95.0           2025-10-29   Good           Water   \n",
       "\n",
       "  dominant_pollutant  vegetation_cover_percentage  \\\n",
       "0               None                         87.0   \n",
       "1               None                         85.0   \n",
       "2               None                         82.0   \n",
       "3               None                         80.0   \n",
       "4               None                         75.0   \n",
       "\n",
       "   renewable_energy_usage_percent  \n",
       "0                            52.0  \n",
       "1                            50.0  \n",
       "2                            48.0  \n",
       "3                            45.0  \n",
       "4                            40.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_inflow_daily non-null fraction: 1.0 dtype: Int64\n",
      "disease_outbreak unique values: disease_outbreak\n",
      "None        109\n",
      "COVID-19     20\n",
      "Dengue       20\n",
      "Typhoid      20\n",
      "Malaria      19\n",
      "Cholera      17\n",
      "Measles      16\n",
      "Name: count, dtype: int64\n",
      "Coverage sample (env + health):\n",
      "            pm25_level  patient_inflow_mean  air_quality_index\n",
      "district                                                      \n",
      "Ahmednagar         1.0                  1.0                1.0\n",
      "Ajra               1.0                  1.0                1.0\n",
      "Akola              1.0                  1.0                1.0\n",
      "Alandi             1.0                  1.0                1.0\n",
      "Alibag             1.0                  1.0                1.0\n",
      "Ambegaon           1.0                  1.0                1.0\n",
      "Ambernath          1.0                  1.0                1.0\n",
      "Amravati           1.0                  1.0                1.0\n",
      "Aurangabad         1.0                  1.0                1.0\n",
      "Badlapur           1.0                  1.0                1.0\n",
      "Outbreak positives: 0  /  9500\n"
     ]
    }
   ],
   "source": [
    "# Inspect health / env columns & dtypes\n",
    "print(\"df_health columns:\", df_health.columns.tolist())\n",
    "print(df_health.dtypes)\n",
    "display(df_health.head(5))\n",
    "\n",
    "print(\"df_env columns:\", df_env.columns.tolist())\n",
    "print(df_env.dtypes)\n",
    "display(df_env.head(5))\n",
    "\n",
    "cands = ['patient_inflow','patient_inflow_daily','Patient Inflow (Daily)','patient_inflow_mean']\n",
    "for c in cands:\n",
    "    if c in df_health.columns:\n",
    "        print(c, \"non-null fraction:\", df_health[c].notna().mean(), \"dtype:\", df_health[c].dtype)\n",
    "        \n",
    "for c in ['disease_outbreak','Disease Outbreak','diseaseOutbreak']:\n",
    "    if c in df_health.columns:\n",
    "        print(c, \"unique values:\", df_health[c].astype(str).value_counts(dropna=False).head(10))\n",
    "        \n",
    "def coverage(panel, cols):\n",
    "    return panel.groupby('district')[cols].apply(lambda s: s.notna().mean()).head(10)\n",
    "print(\"Coverage sample (env + health):\")\n",
    "print(coverage(panel, [c for c in ['pm25_level','patient_inflow_mean','air_quality_index'] if c in panel.columns]))\n",
    "print(\"Outbreak positives:\", panel['outbreak_next_14d'].sum(), \" / \", len(panel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3949ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Disease Outbreak'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Disease Outbreak'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m df_health[date_col] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_health[date_col], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Only consider rows that indicate an outbreak (non-empty Disease Outbreak)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m df_events \u001b[38;5;241m=\u001b[39m df_health[\u001b[43mdf_health\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDisease Outbreak\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnotna() \u001b[38;5;241m&\u001b[39m (df_health[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisease Outbreak\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistrict\u001b[39m\u001b[38;5;124m'\u001b[39m, date_col]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     27\u001b[0m df_events \u001b[38;5;241m=\u001b[39m df_events\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{date_col: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreport_date\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# For each feature_date in df_merged determine if an outbreak occurs within HORIZON_DAYS after feature_date\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Disease Outbreak'"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    " # CELL 4.1: Create forward-looking target & windowed features (Horizon)\n",
    " # ------------------------------------------------------------------\n",
    " import warnings\n",
    " from datetime import timedelta\n",
    "\n",
    " HORIZON_DAYS = 14  # predict outbreak within next 14 days\n",
    "\n",
    " # 1) Forward-looking target from health events\n",
    " date_col = find_date_col(df_health)\n",
    " outbreak_col = first_existing(df_health, ['disease_outbreak', 'Disease Outbreak'])\n",
    " if date_col is None or outbreak_col is None:\n",
    "     warnings.warn('Health table lacks a usable date or outbreak column; skipping forward-looking label build.')\n",
    " elif 'df_merged' not in globals():\n",
    "     warnings.warn('df_merged is not defined; cannot project forward-looking label onto merged feature table.')\n",
    " else:\n",
    "     df_health[date_col] = pd.to_datetime(df_health[date_col], errors='coerce')\n",
    "     outbreak_series = df_health[outbreak_col].astype(str).str.strip().str.lower()\n",
    "     valid_mask = ~outbreak_series.isin({'', 'none', 'null', 'nan'})\n",
    "     df_events = df_health[valid_mask][['district', 'district_norm', date_col]].copy()\n",
    "     df_events = df_events.rename(columns={date_col: 'report_date'})\n",
    "\n",
    "     def has_future_outbreak(row):\n",
    "         d_norm = row.get('district_norm')\n",
    "         if d_norm is None or pd.isna(d_norm):\n",
    "             d_norm = norm_dist(pd.Series([row.get('district', '')])).iloc[0]\n",
    "         t0 = row.get('feature_date')\n",
    "         if d_norm is None or pd.isna(t0):\n",
    "             return 0\n",
    "         ev = df_events[df_events['district_norm'] == d_norm]['report_date']\n",
    "         if ev.empty:\n",
    "             return 0\n",
    "         future = ev[(ev > t0) & (ev <= t0 + pd.Timedelta(days=HORIZON_DAYS))]\n",
    "         return int(not future.empty)\n",
    "\n",
    "     df_merged['district_norm'] = norm_dist(df_merged['district']) if 'district_norm' not in df_merged.columns else df_merged['district_norm']\n",
    "     df_merged['outbreak_future_14d'] = df_merged.apply(has_future_outbreak, axis=1)\n",
    "     df_merged['outbreak_risk'] = df_merged['outbreak_future_14d']\n",
    "     print(f\"Created forward-looking label outbreak_future_14d using {date_col} / {outbreak_col}\")\n",
    "\n",
    " # 2) Parse crime JSON into district-level columns (optional)\n",
    " if 'crime_reports' in df_safety.columns and 'df_merged' in globals():\n",
    "     def _parse_crime_json(x):\n",
    "         try:\n",
    "             if pd.isna(x):\n",
    "                 return {}\n",
    "             if isinstance(x, str):\n",
    "                 parsed = json.loads(x)\n",
    "             else:\n",
    "                 parsed = x\n",
    "         except Exception:\n",
    "             return {}\n",
    "         counts = {}\n",
    "         if isinstance(parsed, dict):\n",
    "             parsed = [parsed]\n",
    "         if isinstance(parsed, (list, tuple)):\n",
    "             for item in parsed:\n",
    "                 if not isinstance(item, dict):\n",
    "                     continue\n",
    "                 ctype = item.get('type')\n",
    "                 cval = item.get('count', 1)\n",
    "                 try:\n",
    "                     cval = int(float(cval))\n",
    "                 except Exception:\n",
    "                     cval = 0\n",
    "                 if ctype and cval:\n",
    "                     counts[ctype] = counts.get(ctype, 0) + cval\n",
    "         return counts\n",
    "\n",
    "     crime_expanded = df_safety[['district_norm', 'crime_reports']].copy()\n",
    "     crime_expanded['crime_counts'] = crime_expanded['crime_reports'].apply(_parse_crime_json)\n",
    "     crime_cols = set()\n",
    "     for counts in crime_expanded['crime_counts']:\n",
    "         crime_cols.update(counts.keys())\n",
    "     for crime in crime_cols:\n",
    "         df_merged[f'crime_{crime}'] = df_merged['district_norm'].map(\n",
    "             crime_expanded.set_index('district_norm')['crime_counts'].apply(lambda d: d.get(crime, 0) if isinstance(d, dict) else 0)\n",
    "         )\n",
    "     print(f'Merged crime JSON counts into df_merged ({len(crime_cols)} columns).')\n",
    " elif 'df_merged' not in globals():\n",
    "     warnings.warn('df_merged is not defined; skipping crime_reports feature engineering.')\n",
    " else:\n",
    "     print('No crime_reports column available to parse.')\n",
    "\n",
    " # 3) Placeholder for rolling environmental joins (requires df_merged with feature_date)\n",
    " env_date_col = find_date_col(df_env)\n",
    " if env_date_col and 'df_merged' in globals():\n",
    "     df_env[env_date_col] = pd.to_datetime(df_env[env_date_col], errors='coerce')\n",
    "     env_weekly = df_env.copy()\n",
    "     env_weekly['week_start'] = norm_week(env_weekly[env_date_col])\n",
    "     env_metrics = env_weekly.groupby(['district_norm', 'week_start']).agg({\n",
    "         'pm25_level': 'mean',\n",
    "         'pm10_level': 'mean',\n",
    "         'air_quality_index': 'mean'\n",
    "     }).reset_index()\n",
    "     df_merged['feature_week'] = norm_week(df_merged['feature_date'])\n",
    "     df_merged = df_merged.merge(\n",
    "         env_metrics.rename(columns={'week_start': 'feature_week'}),\n",
    "         on=['district_norm', 'feature_week'],\n",
    "         how='left'\n",
    "     )\n",
    "     print('Merged weekly environmental metrics onto df_merged (demo).')\n",
    " elif 'df_merged' not in globals():\n",
    "     warnings.warn('df_merged missing; skipping environmental feature join.')\n",
    " else:\n",
    "     print('No date column in environment_monitoring; skipping environmental window features.')\n",
    "\n",
    " print('Horizon & windowed feature step complete (demo skeleton).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a226f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# CELL 5: Preprocess Data (use df_model from district-week panel)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Select limited, meaningful features for modeling (as-of at week_start)\n",
    "relevant_features = [\n",
    "    'pm25_level', 'pm10_level', 'air_quality_index', 'water_quality_index', 'waste_collection_efficiency',\n",
    "    'population_density_per_sqkm', 'avg_household_size',\n",
    "    'condition_score', 'traffic_volume_daily',\n",
    "    'patient_inflow_mean',\n",
    "    # rolling features\n",
    "    'pm25_roll_1w', 'pm25_roll_2w', 'pm25_roll_4w',\n",
    "    'patient_inflow_roll_1w','patient_inflow_roll_2w','patient_inflow_roll_4w'\n",
    "]\n",
    "\n",
    "# Ensure features exist (some env columns may not be present)\n",
    "relevant_features = [f for f in relevant_features if f in df_model.columns]\n",
    "print('Using features:', relevant_features)\n",
    "\n",
    "# Target\n",
    "target_col = 'outbreak_next_14d'\n",
    "X_df = df_model[['district','week_start'] + relevant_features].copy()\n",
    "y = df_model[target_col].copy()\n",
    "\n",
    "# Drop initial rows with NaN in key features\n",
    "X_df = X_df.dropna(subset=relevant_features)\n",
    "# align y\n",
    "y = y.loc[X_df.index]\n",
    "\n",
    "# Split by time: train up to a date, test after\n",
    "cutoff = X_df['week_start'].quantile(0.8)\n",
    "train_mask = X_df['week_start'] <= cutoff\n",
    "X_train_raw = X_df[train_mask].drop(columns=['district','week_start'])\n",
    "X_test_raw = X_df[~train_mask].drop(columns=['district','week_start'])\n",
    "y_train = y[train_mask]\n",
    "y_test = y[~train_mask]\n",
    "\n",
    "print(f'Train weeks: {X_df[train_mask][\"week_start\"].min()} to {X_df[train_mask][\"week_start\"].max()}')\n",
    "print(f'Test weeks: {X_df[~train_mask][\"week_start\"].min()} to {X_df[~train_mask][\"week_start\"].max()}')\n",
    "\n",
    "# Preprocessing: numeric only for now\n",
    "numerical_cols = X_train_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols)])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train_raw)\n",
    "X_test = preprocessor.transform(X_test_raw)\n",
    "\n",
    "print('Final feature matrix shapes:', X_train.shape, X_test.shape)\n",
    "print('Train positive rate:', y_train.mean(), 'Test positive rate:', y_test.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea3993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# CELL 6: Data Exploration (Split moved to CELL 5)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "print(\"Time-based data split completed in CELL 5 to avoid data leakage.\")\n",
    "print(f\"Train period: {X_df[train_mask]['week_start'].min()} to {X_df[train_mask]['week_start'].max()}\")\n",
    "print(f\"Test period: {X_df[~train_mask]['week_start'].min()} to {X_df[~train_mask]['week_start'].max()}\")\n",
    "print('Number of districts in panel:', df_model['district'].nunique())\n",
    "print('Date range in panel:', df_model['week_start'].min(), 'to', df_model['week_start'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904bb9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# CELL 7: Choose Best Algorithm (class-weighting / scale_pos_weight) + Calibration\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Split training into train_inner / val for calibration (last 10% of training as val)\n",
    "n_train = len(X_train)\n",
    "val_n = max(1, int(0.1 * n_train))\n",
    "train_inner_n = n_train - val_n\n",
    "X_train_inner = X_train[:train_inner_n]\n",
    "X_val = X_train[train_inner_n:]\n",
    "y_train_inner = y_train.iloc[:train_inner_n]\n",
    "y_val = y_train.iloc[train_inner_n:]\n",
    "\n",
    "# Compute class weight / scale\n",
    "pos = y_train.sum()\n",
    "neg = len(y_train) - pos\n",
    "scale_pos_weight = (neg / pos) if pos>0 else 1.0\n",
    "print('scale_pos_weight (neg/pos):', scale_pos_weight)\n",
    "\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(objective='binary:logistic', scale_pos_weight=scale_pos_weight, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42)\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_auc = 0\n",
    "best_name = ''\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Fit on inner training\n",
    "    model.fit(X_train_inner, y_train_inner)\n",
    "    # Calibrate on val using prefit\n",
    "    try:\n",
    "        calib = CalibratedClassifierCV(model, cv='prefit', method='sigmoid')\n",
    "        calib.fit(X_val, y_val)\n",
    "        model_to_eval = calib\n",
    "    except Exception:\n",
    "        model_to_eval = model\n",
    "    # Evaluate on test\n",
    "    y_pred = model_to_eval.predict(X_test)\n",
    "    y_pred_proba = model_to_eval.predict_proba(X_test)[:, 1]\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    brier = brier_score_loss(y_test, y_pred_proba)\n",
    "    print(f\"{name} - F1-Score: {f1:.4f}, ROC AUC: {auc:.4f}, PR AUC: {pr_auc:.4f}, Brier: {brier:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # recall@K: top-K by predicted probability (e.g., top 10% weeks)\n",
    "    k = max(1, int(0.1 * len(y_test)))\n",
    "    topk_idx = np.argsort(y_pred_proba)[-k:]\n",
    "    recall_at_k = y_test.iloc[topk_idx].sum() / y_test.sum() if y_test.sum()>0 else 0.0\n",
    "    print(f'Recall@{k} (top 10%): {recall_at_k:.3f}')\n",
    "\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_model = model\n",
    "        best_name = name\n",
    "\n",
    "print(f\"\\nBest model: {best_name} with ROC AUC {best_auc:.4f}\")\n",
    "\n",
    "# Retrain best model on full training data and calibrate using last 10% of original training\n",
    "best_model.fit(X_train, y_train)\n",
    "calibrated = None\n",
    "try:\n",
    "    val_for_cal = X_train[-val_n:]\n",
    "    y_for_cal = y_train.iloc[-val_n:]\n",
    "    calibrated = CalibratedClassifierCV(best_model, cv='prefit', method='sigmoid')\n",
    "    calibrated.fit(val_for_cal, y_for_cal)\n",
    "    final_model = calibrated\n",
    "    print('Calibration applied to final model.')\n",
    "except Exception as e:\n",
    "    final_model = best_model\n",
    "    print('Calibration skipped (fallback to raw model):', e)\n",
    "\n",
    "# Feature importance for tree models\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    try:\n",
    "        # try to recover feature names from numerical_cols\n",
    "        feature_names = numerical_cols\n",
    "    except Exception:\n",
    "        feature_names = [f'feat_{i}' for i in range(X_train.shape[1])]\n",
    "    importances = best_model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    print(feature_importance_df.head(10))\n",
    "\n",
    "# Save final_model reference for downstream cells\n",
    "best_model = final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958889ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# CELL 8: Train Final Model and Save (artifact at root) \n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Create full pipeline with preprocessor and classifier\n",
    "final_clf = type(best_model)(**best_model.get_params())\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', final_clf)\n",
    "])\n",
    "\n",
    "# Fit on full training set (X_train_raw aligned earlier)\n",
    "full_pipeline.fit(X_train_raw, y_train)\n",
    "\n",
    "# Save artifact at repo root with standard name expected by sklearn container\n",
    "import joblib\n",
    "joblib.dump(full_pipeline, 'model.joblib')\n",
    "print('Saved pipeline as model.joblib')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CELL 9: Upload to GCS and Deploy Vertex AI Endpoint (demo)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Require user to set a BUCKET variable\n",
    "BUCKET = 'your-bucket'  # <-- Replace this with your GCS bucket name or set via environment\n",
    "if BUCKET == 'your-bucket':\n",
    "    print('Please set BUCKET to your real GCS bucket before running deployment steps. Skipping upload/deploy.')\n",
    "else:\n",
    "    # Upload to GCS root of model-artifacts\n",
    "    !gsutil cp model.joblib gs://{BUCKET}/model.joblib\n",
    "\n",
    "    # Upload to Vertex AI and deploy\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=MODEL_DISPLAY_NAME,\n",
    "        artifact_uri=f'gs://{BUCKET}/',\n",
    "        serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest',\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION\n",
    "    )\n",
    "\n",
    "    endpoint = model.deploy(machine_type='n1-standard-2', min_replica_count=1, max_replica_count=1)\n",
    "    print(f'Deployed model to endpoint: {endpoint.resource_name}')\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
